# Prometheus Alert Rules for Clipper
# Place this file in /opt/clipper/monitoring/alerts.yml
#
# These alerts are based on defined SLOs:
# - Availability: 99.5% uptime
# - Latency: P95 < 100ms (list), P95 < 50ms (detail)
# - Error Rate: < 0.5%
#
# Related Issues (Roadmap 5.0 - Phase 5.3):
# - #860 - Alerting Configuration
# - #858 - Grafana Dashboards (metrics alignment)
# - #805 - Observability Infrastructure

groups:
  - name: clipper_slo_alerts
    interval: 30s
    rules:
      # SLO: Availability - 99.5% uptime
      - alert: SLOAvailabilityBreach
        expr: |
          (
            1 - (sum(rate(http_requests_total{status=~"2.."}[5m])) / sum(rate(http_requests_total[5m])))
          ) > 0.005
        for: 5m
        labels:
          severity: critical
          slo: availability
        annotations:
          summary: "Availability SLO breach"
          description: "Service availability is {{ $value | humanizePercentage }}, target is 99.5%."
          runbook: "Check service health, investigate errors, consider rollback."

      # SLO: Error Rate - < 0.5%
      - alert: SLOErrorRateBreach
        expr: |
          (
            sum(rate(http_requests_total{status=~"5.."}[5m])) / sum(rate(http_requests_total[5m]))
          ) > 0.005
        for: 5m
        labels:
          severity: critical
          slo: error_rate
        annotations:
          summary: "Error rate SLO breach"
          description: "Error rate is {{ $value | humanizePercentage }}, target is < 0.5%."
          runbook: "Check error logs, identify error patterns, apply fix or rollback."

      # SLO: Latency - P95 < 100ms for list endpoints
      - alert: SLOLatencyBreach
        expr: |
          histogram_quantile(0.95,
            sum(rate(http_request_duration_seconds_bucket{path=~"/api/v1/(clips|feed|lists).*"}[5m])) by (le)
          ) > 0.1
        for: 5m
        labels:
          severity: warning
          slo: latency
        annotations:
          summary: "Latency SLO breach for list endpoints"
          description: "P95 latency is {{ $value }}s, target is < 0.1s."
          runbook: "Check database query performance, Redis cache hit rate, system resources."

      # Error Budget: Fast burn (> 10% in 1 hour)
      - alert: ErrorBudgetFastBurn
        expr: |
          (
            1 - (sum(rate(http_requests_total{status=~"2.."}[1h])) / sum(rate(http_requests_total[1h])))
          ) > 0.1
        for: 5m
        labels:
          severity: critical
          error_budget: fast_burn
        annotations:
          summary: "Error budget fast burn detected"
          description: "Consuming > 10% error budget in 1 hour. Current availability: {{ $value | humanizePercentage }}."
          runbook: "Immediate action required. Investigate and mitigate ongoing issues."

      # Error Budget: Medium burn (> 25% in 6 hours)
      - alert: ErrorBudgetMediumBurn
        expr: |
          (
            1 - (sum(rate(http_requests_total{status=~"2.."}[6h])) / sum(rate(http_requests_total[6h])))
          ) > 0.25
        for: 15m
        labels:
          severity: warning
          error_budget: medium_burn
        annotations:
          summary: "Error budget medium burn detected"
          description: "Consuming > 25% error budget in 6 hours. Current availability: {{ $value | humanizePercentage }}."
          runbook: "Review recent changes, implement stability improvements."

  - name: clipper_service_alerts
    interval: 30s
    rules:
      # Service Down
      - alert: ServiceDown
        expr: up == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Service {{ $labels.job }} is down"
          description: "{{ $labels.job }} has been down for more than 1 minute."

      # High Error Rate
      - alert: HighErrorRate
        expr: |
          (
            sum(rate(http_requests_total{status=~"5.."}[5m])) by (job)
            /
            sum(rate(http_requests_total[5m])) by (job)
          ) > 0.05
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High error rate on {{ $labels.job }}"
          description: "{{ $labels.job }} has an error rate of {{ $value | humanizePercentage }} over the last 5 minutes."

      # Critical Error Rate (P1)
      - alert: CriticalErrorRate
        expr: |
          (
            sum(rate(http_requests_total{status=~"5.."}[5m])) by (job)
            /
            sum(rate(http_requests_total[5m])) by (job)
          ) > 0.01
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Critical error rate on {{ $labels.job }}"
          description: "{{ $labels.job }} has a critical error rate of {{ $value | humanizePercentage }} (threshold: > 1%)."

      # High Response Time
      - alert: HighResponseTime
        expr: |
          histogram_quantile(0.95,
            sum(rate(http_request_duration_seconds_bucket[5m])) by (job, le)
          ) > 0.5
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High response time on {{ $labels.job }}"
          description: "{{ $labels.job }} 95th percentile response time is {{ $value }}s."

  - name: clipper_resource_alerts
    interval: 30s
    rules:
      # High CPU Usage
      - alert: HighCPUUsage
        expr: |
          100 - (avg by (instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High CPU usage on {{ $labels.instance }}"
          description: "CPU usage is {{ $value | humanize }}% on {{ $labels.instance }}."

      # High Memory Usage
      - alert: HighMemoryUsage
        expr: |
          (
            node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes
          ) / node_memory_MemTotal_bytes * 100 > 80
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High memory usage on {{ $labels.instance }}"
          description: "Memory usage is {{ $value | humanize }}% on {{ $labels.instance }}."

      # Low Disk Space
      - alert: LowDiskSpace
        expr: |
          (
            node_filesystem_avail_bytes{mountpoint="/"}
            / node_filesystem_size_bytes{mountpoint="/"}
          ) * 100 < 20
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Low disk space on {{ $labels.instance }}"
          description: "Disk space is {{ $value | humanize }}% free on {{ $labels.instance }}."

      # Critical Disk Space
      - alert: CriticalDiskSpace
        expr: |
          (
            node_filesystem_avail_bytes{mountpoint="/"}
            / node_filesystem_size_bytes{mountpoint="/"}
          ) * 100 < 10
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Critical disk space on {{ $labels.instance }}"
          description: "Disk space is {{ $value | humanize }}% free on {{ $labels.instance }}."

  - name: clipper_database_alerts
    interval: 30s
    rules:
      # Database Connection Issues
      - alert: DatabaseConnectionIssues
        expr: |
          sum(pg_stat_database_numbackends) by (datname) > 80
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High number of database connections"
          description: "Database {{ $labels.datname }} has {{ $value }} active connections."

      # Database Down
      - alert: DatabaseDown
        expr: pg_up == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "PostgreSQL is down"
          description: "PostgreSQL database is not responding."

      # Slow Queries
      - alert: SlowQueries
        expr: |
          rate(pg_stat_activity_max_tx_duration[5m]) > 30
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Slow database queries detected"
          description: "Database has queries taking more than 30 seconds."

  - name: clipper_redis_alerts
    interval: 30s
    rules:
      # Redis Down
      - alert: RedisDown
        expr: redis_up == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Redis is down"
          description: "Redis cache is not responding."

      # High Redis Memory Usage
      - alert: HighRedisMemoryUsage
        expr: |
          (redis_memory_used_bytes / redis_memory_max_bytes) * 100 > 80
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High Redis memory usage"
          description: "Redis is using {{ $value | humanize }}% of allocated memory."

      # Low Cache Hit Rate
      - alert: LowCacheHitRate
        expr: |
          (
            rate(redis_keyspace_hits_total[5m]) + rate(redis_keyspace_misses_total[5m])
          ) > 0.5
          and (
            rate(redis_keyspace_hits_total[5m])
            / (rate(redis_keyspace_hits_total[5m]) + rate(redis_keyspace_misses_total[5m]))
          ) < 0.8
        for: 30m
        labels:
          severity: info
        annotations:
          summary: "Low Redis cache hit rate"
          description: "Redis cache hit rate is {{ $value | humanizePercentage }} (only alerts when >0.5 req/s traffic)."

  - name: clipper_ssl_alerts
    interval: 1d
    rules:
      # SSL Certificate Expiring Soon
      - alert: SSLCertificateExpiringSoon
        expr: |
          (ssl_certificate_expiry_seconds - time()) / 86400 < 30
        for: 1h
        labels:
          severity: warning
        annotations:
          summary: "SSL certificate expiring soon"
          description: "SSL certificate for {{ $labels.domain }} expires in {{ $value }} days."

      # SSL Certificate Expired
      - alert: SSLCertificateExpired
        expr: |
          (ssl_certificate_expiry_seconds - time()) < 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "SSL certificate expired"
          description: "SSL certificate for {{ $labels.domain }} has expired."

  - name: clipper_search_alerts
    interval: 30s
    rules:
      # Semantic Search High Latency
      - alert: SemanticSearchHighLatency
        expr: |
          histogram_quantile(0.95, sum(rate(search_query_duration_ms_bucket{search_type="hybrid"}[5m])) by (le)) > 200
        for: 5m
        labels:
          severity: warning
          slo: search_latency
        annotations:
          summary: "Semantic search P95 latency exceeds 200ms"
          description: "Hybrid search P95 latency is {{ $value }}ms, target is < 200ms."
          runbook: "docs/operations/playbooks/search-incidents.md#high-latency"

      # Search Critical Latency
      - alert: SemanticSearchCriticalLatency
        expr: |
          histogram_quantile(0.95, sum(rate(search_query_duration_ms_bucket{search_type="hybrid"}[5m])) by (le)) > 500
        for: 3m
        labels:
          severity: critical
          slo: search_latency
        annotations:
          summary: "Semantic search P95 latency critical (>500ms)"
          description: "Hybrid search P95 latency is {{ $value }}ms. Immediate action required."
          runbook: "docs/operations/playbooks/search-incidents.md#critical-latency"

      # Embedding Generation Failures
      - alert: EmbeddingGenerationFailing
        expr: |
          rate(embedding_generation_errors_total[5m]) > 0.1
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Embedding generation failing"
          description: "Embedding generation error rate is {{ $value }}/sec. This affects semantic search quality."
          runbook: "docs/operations/playbooks/search-incidents.md#embedding-failures"

      # Low Embedding Coverage
      - alert: LowEmbeddingCoverage
        expr: |
          (clips_with_embeddings + clips_without_embeddings) > 0
          and clips_without_embeddings / (clips_with_embeddings + clips_without_embeddings) > 0.1
        for: 30m
        labels:
          severity: warning
        annotations:
          summary: "Low embedding coverage"
          description: "{{ $value | humanizePercentage }} of clips are missing embeddings (threshold: 10%)."
          runbook: "docs/operations/playbooks/search-incidents.md#low-coverage"

      # Embedding Cache Low Hit Rate
      - alert: EmbeddingCacheLowHitRate
        expr: |
          (rate(embedding_cache_hits_total[5m]) + rate(embedding_cache_misses_total[5m])) > 0
          and (rate(embedding_cache_hits_total[5m]) / (rate(embedding_cache_hits_total[5m]) + rate(embedding_cache_misses_total[5m]))) < 0.5
        for: 15m
        labels:
          severity: warning
        annotations:
          summary: "Embedding cache hit rate below 50%"
          description: "Embedding cache hit rate is {{ $value | humanizePercentage }}. This increases API costs and latency."
          runbook: "docs/operations/playbooks/search-incidents.md#cache-issues"

      # High Zero Result Rate
      - alert: HighZeroResultRate
        expr: |
          rate(search_queries_total[5m]) > 0
          and rate(search_zero_results_total[5m]) / rate(search_queries_total[5m]) > 0.1
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "High zero result rate in search"
          description: "{{ $value | humanizePercentage }} of searches return no results."
          runbook: "docs/operations/playbooks/search-incidents.md#zero-results"

      # Search Fallback Activated
      - alert: SearchFallbackActivated
        expr: |
          rate(search_fallback_total[5m]) > 0.5
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Search falling back to BM25 frequently"
          description: "Search is falling back to BM25-only at {{ $value }}/sec due to {{ $labels.reason }}."
          runbook: "docs/operations/playbooks/search-incidents.md#fallback-issues"

      # Indexing Job Failures
      - alert: IndexingJobFailing
        expr: |
          increase(indexing_jobs_total{status="failed"}[1h]) > 3
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Indexing jobs failing repeatedly"
          description: "{{ $value }} indexing jobs failed in the last hour."
          runbook: "docs/operations/playbooks/search-incidents.md#indexing-failures"

      # Vector Search Slow
      - alert: VectorSearchSlow
        expr: |
          histogram_quantile(0.95, sum(rate(vector_search_duration_ms_bucket[5m])) by (le)) > 100
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Vector re-ranking is slow"
          description: "Vector search P95 latency is {{ $value }}ms, target is < 100ms."
          runbook: "docs/operations/playbooks/search-incidents.md#vector-search-slow"

      # BM25 Search Slow
      - alert: BM25SearchSlow
        expr: |
          histogram_quantile(0.95, sum(rate(bm25_search_duration_ms_bucket[5m])) by (le)) > 50
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "BM25 candidate search is slow"
          description: "BM25 search P95 latency is {{ $value }}ms, target is < 50ms."
          runbook: "docs/operations/playbooks/search-incidents.md#bm25-search-slow"

      # Search Failover Rate High
      - alert: SearchFailoverRateHigh
        expr: |
          rate(search_fallback_total[5m]) > 5
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High search failover rate"
          description: "Search is failing over at {{ $value }}/sec due to {{ $labels.reason }}. OpenSearch may be degraded."
          runbook: "docs/operations/playbooks/search-incidents.md#search-failover"

      # Search Failover Rate Critical
      - alert: SearchFailoverRateCritical
        expr: |
          rate(search_fallback_total[5m]) > 20
        for: 3m
        labels:
          severity: critical
        annotations:
          summary: "Critical search failover rate"
          description: "Search is failing over at {{ $value }}/sec due to {{ $labels.reason }}. Immediate action required."
          runbook: "docs/operations/playbooks/search-incidents.md#search-failover"

      # Search Failover Latency High
      - alert: SearchFailoverLatencyHigh
        expr: |
          histogram_quantile(0.95, sum(rate(search_fallback_duration_ms_bucket[5m])) by (le)) > 500
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High search fallback latency"
          description: "P95 fallback path latency is {{ $value }}ms, may indicate PostgreSQL performance issues."
          runbook: "docs/operations/playbooks/search-incidents.md#fallback-latency"

  - name: clipper_webhook_alerts
    interval: 30s
    rules:
      # High Webhook Failure Rate
      - alert: HighWebhookFailureRate
        expr: |
          (
            sum(rate(webhook_delivery_total{status="failed"}[5m]))
            /
            sum(rate(webhook_delivery_total[5m]))
          ) > 0.1
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "High webhook delivery failure rate"
          description: "Webhook delivery failure rate is {{ $value | humanizePercentage }}, target is < 10%."
          runbook: "docs/backend/webhooks.md#troubleshooting"

      # Critical Webhook Failure Rate
      - alert: CriticalWebhookFailureRate
        expr: |
          (
            sum(rate(webhook_delivery_total{status="failed"}[5m]))
            /
            sum(rate(webhook_delivery_total[5m]))
          ) > 0.5
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Critical webhook delivery failure rate"
          description: "Webhook delivery failure rate is {{ $value | humanizePercentage }} (threshold: 50%)."
          runbook: "docs/backend/webhooks.md#troubleshooting"

      # Large Webhook Retry Queue
      - alert: LargeWebhookRetryQueue
        expr: webhook_retry_queue_size > 100
        for: 15m
        labels:
          severity: warning
        annotations:
          summary: "Large webhook retry queue"
          description: "Webhook retry queue has {{ $value }} items pending."
          runbook: "docs/backend/webhook-retry.md#high-retry-queue-size"

      # Critical Webhook Retry Queue
      - alert: CriticalWebhookRetryQueue
        expr: webhook_retry_queue_size > 500
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Critical webhook retry queue size"
          description: "Webhook retry queue has {{ $value }} items pending. Immediate action required."
          runbook: "docs/backend/webhook-retry.md#high-retry-queue-size"

      # Webhook Dead-Letter Queue Items
      - alert: WebhookDeadLetterQueueItems
        expr: webhook_dead_letter_queue_size > 10
        for: 30m
        labels:
          severity: warning
        annotations:
          summary: "Webhooks in dead-letter queue"
          description: "{{ $value }} webhook deliveries have failed permanently and are in the dead-letter queue."
          runbook: "docs/backend/webhook-retry.md#events-in-dlq"

      # Critical Webhook Dead-Letter Queue
      - alert: CriticalWebhookDeadLetterQueue
        expr: webhook_dead_letter_queue_size > 50
        for: 15m
        labels:
          severity: critical
        annotations:
          summary: "Critical number of webhooks in dead-letter queue"
          description: "{{ $value }} webhook deliveries have failed permanently. Investigation required."
          runbook: "docs/backend/webhook-retry.md#events-in-dlq"

      # High Webhook Delivery Latency
      - alert: HighWebhookDeliveryLatency
        expr: |
          histogram_quantile(0.95,
            sum(rate(webhook_delivery_duration_seconds_bucket{status="success"}[5m])) by (le)
          ) > 5
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "High webhook delivery latency"
          description: "P95 webhook delivery latency is {{ $value }}s, target is < 5s."
          runbook: "docs/backend/webhooks.md#troubleshooting"

      # Webhook Delivery Latency Spike
      - alert: WebhookDeliveryLatencySpike
        expr: |
          (
            histogram_quantile(0.95,
              sum(rate(webhook_delivery_duration_seconds_bucket{status="success"}[5m])) by (le)
            )
            /
            histogram_quantile(0.95,
              sum(rate(webhook_delivery_duration_seconds_bucket{status="success"}[1h])) by (le)
            )
          ) > 2
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Webhook delivery latency spike detected"
          description: "P95 latency has increased by {{ humanize $value }}x compared to 1-hour baseline."
          runbook: "docs/backend/webhooks.md#troubleshooting"

      # High DLQ Movement Rate
      - alert: HighDLQMovementRate
        expr: |
          sum(rate(webhook_dlq_movements_total[5m])) > 1
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "High rate of webhooks moving to DLQ"
          description: "{{ $value }} webhooks/sec are being moved to the dead-letter queue."
          runbook: "docs/backend/webhook-retry.md#high-dlq-movement-rate"

      # Critical DLQ Movement Rate
      - alert: CriticalDLQMovementRate
        expr: |
          sum(rate(webhook_dlq_movements_total[5m])) > 5
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Critical rate of webhooks moving to DLQ"
          description: "{{ $value }} webhooks/sec are being moved to the dead-letter queue. Immediate investigation required."
          runbook: "docs/backend/webhook-retry.md#high-dlq-movement-rate"

      # Subscription Consecutive Failures
      - alert: WebhookSubscriptionConsecutiveFailures
        expr: webhook_consecutive_failures_count > 5
        for: 15m
        labels:
          severity: warning
        annotations:
          summary: "Webhook subscription experiencing consecutive failures"
          description: "Subscription {{ $labels.subscription_id }} for event {{ $labels.event_type }} has {{ $value }} consecutive failures."
          runbook: "docs/backend/webhooks.md#subscription-health"

      # Subscription Critical Consecutive Failures
      - alert: WebhookSubscriptionCriticalFailures
        expr: webhook_consecutive_failures_count > 20
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Webhook subscription critically failing"
          description: "Subscription {{ $labels.subscription_id }} for event {{ $labels.event_type }} has {{ $value }} consecutive failures. Subscription may need attention."
          runbook: "docs/backend/webhooks.md#subscription-health"

      # Subscription Health Degradation
      - alert: WebhookSubscriptionHealthDegradation
        expr: |
          (
            sum(rate(webhook_subscription_delivery_total{status="failed"}[5m])) by (subscription_id)
            /
            sum(rate(webhook_subscription_delivery_total[5m])) by (subscription_id)
          ) > 0.5
        for: 15m
        labels:
          severity: warning
        annotations:
          summary: "Webhook subscription health degraded"
          description: "Subscription {{ $labels.subscription_id }} has {{ $value | humanizePercentage }} failure rate."
          runbook: "docs/backend/webhooks.md#subscription-health"

      # High Retry Exhaustion Rate
      - alert: HighRetryExhaustionRate
        expr: |
          (
            sum(rate(webhook_retry_attempts_count{final_status="failed"}[5m]))
            /
            sum(rate(webhook_retry_attempts_count[5m]))
          ) * 100 > 30
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "High webhook retry exhaustion rate"
          description: "{{ $value | humanizePercentage }} of webhook retries are exhausting all attempts."
          runbook: "docs/backend/webhook-retry.md#high-exhaustion-rate"

      # Webhook Delivery Stalled
      - alert: WebhookDeliveryStalled
        expr: |
          rate(webhook_delivery_total[5m]) == 0
          and webhook_retry_queue_size > 0
        for: 10m
        labels:
          severity: critical
        annotations:
          summary: "Webhook delivery processing appears stalled"
          description: "No webhook deliveries processed in 10 minutes, but retry queue has items."
          runbook: "docs/backend/webhooks.md#delivery-stalled"

      # No Active Webhook Subscriptions (informational)
      - alert: NoActiveWebhookSubscriptions
        expr: webhook_subscriptions_active == 0
        for: 1h
        labels:
          severity: info
        annotations:
          summary: "No active webhook subscriptions"
          description: "There are currently no active webhook subscriptions configured."
          runbook: "docs/backend/webhooks.md"

  - name: clipper_logging_alerts
    interval: 30s
    rules:
      # High Error Log Rate
      - alert: HighErrorLogRate
        expr: |
          sum(rate({level="error"}[5m])) by (service) > 10
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High error log rate in {{ $labels.service }}"
          description: "{{ $labels.service }} is generating {{ $value }} error logs per second."
          runbook: "Check logs for error patterns and investigate root cause."

      # Critical Error Spike
      - alert: CriticalErrorSpike
        expr: |
          sum(rate({level="error"}[5m])) by (service) > 50
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Critical error spike in {{ $labels.service }}"
          description: "{{ $labels.service }} is experiencing {{ $value }} errors/sec. Immediate action required."
          runbook: "Investigate error patterns, consider rollback if recent deployment."

      # Security Event: Failed Authentication
      - alert: FailedAuthenticationSpike
        expr: |
          sum(rate({message=~".*authentication failed.*|.*login failed.*|.*unauthorized.*"}[5m])) by (service) > 5
        for: 3m
        labels:
          severity: warning
          security: true
        annotations:
          summary: "Failed authentication spike in {{ $labels.service }}"
          description: "{{ $labels.service }} has {{ $value }} failed auth attempts per second."
          runbook: "Check for brute force attacks, review IP addresses, consider rate limiting."

      # Security Event: SQL Injection Attempt
      - alert: SQLInjectionAttempt
        expr: |
          count_over_time({level="error",message=~".*(?i)(union.*select|insert.*into|drop.*table|delete.*from).*"}[5m]) > 0
        for: 1m
        labels:
          severity: critical
          security: true
        annotations:
          summary: "Potential SQL injection attempt detected"
          description: "{{ $value }} potential SQL injection patterns detected in logs."
          runbook: "Review logs immediately, block suspicious IPs, verify input validation."

      # Security Event: Suspicious Activity
      - alert: SuspiciousSecurityEvent
        expr: |
          sum(rate({message=~".*(?i)(security|breach|intrusion|malicious|exploit).*",level="warn"}[5m])) by (service) > 1
        for: 2m
        labels:
          severity: critical
          security: true
        annotations:
          summary: "Suspicious security events in {{ $labels.service }}"
          description: "{{ $labels.service }} has {{ $value }} security-related warnings per second."
          runbook: "Investigate security logs, check for breach indicators, notify security team."

      # Application Panic/Crash
      - alert: ApplicationPanic
        expr: |
          count_over_time({message=~".*(?i)(panic|crash|fatal).*"}[5m]) > 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Application panic detected in {{ $labels.service }}"
          description: "{{ $value }} panic/crash events detected. Application may be unstable."
          runbook: "Check stack traces, review recent deployments, consider rollback."

      # No Logs Received
      - alert: NoLogsReceived
        expr: |
          absent_over_time(({job="backend"} or {job="frontend"})[10m])
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "No logs received from {{ $labels.job }}"
          description: "No logs have been received from {{ $labels.job }} for 10 minutes."
          runbook: "Check if application is running, verify log shipping configuration."

      # High Log Volume
      - alert: HighLogVolume
        expr: |
          sum(rate({job=~"backend|frontend"}[5m])) by (service) > 1000
        for: 10m
        labels:
          severity: info
        annotations:
          summary: "High log volume from {{ $labels.service }}"
          description: "{{ $labels.service }} is generating {{ $value }} logs per second."
          runbook: "Review if this is expected behavior or indicates an issue."

      # Repeated Error Pattern (optimized query)
      - alert: RepeatedErrorPattern
        expr: |
          count(count_over_time({level="error",service=~".+"} [1h])) by (service) > 100
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Repeated error pattern detected in {{ $labels.service }}"
          description: "{{ $labels.service }} has {{ $value }} error occurrences in the last hour."
          runbook: "Investigate root cause of repeated errors in the service."

      # Database Connection Errors in Logs
      - alert: DatabaseConnectionErrors
        expr: |
          sum(rate({message=~".*(?i)(database.*connection.*failed|postgres.*error|connection.*refused).*",level="error"}[5m])) by (service) > 1
        for: 3m
        labels:
          severity: critical
        annotations:
          summary: "Database connection errors in {{ $labels.service }}"
          description: "{{ $labels.service }} has {{ $value }} database connection errors per second."
          runbook: "Check database health, verify connection pool configuration."

      # Redis Connection Errors in Logs
      - alert: RedisConnectionErrors
        expr: |
          sum(rate({message=~".*(?i)(redis.*connection.*failed|redis.*error|redis.*timeout).*",level="error"}[5m])) by (service) > 1
        for: 3m
        labels:
          severity: warning
        annotations:
          summary: "Redis connection errors in {{ $labels.service }}"
          description: "{{ $labels.service }} has {{ $value }} Redis connection errors per second."
          runbook: "Check Redis health, verify connection configuration."

  - name: clipper_background_jobs_alerts
    interval: 30s
    rules:
      # Job Execution Failures
      - alert: BackgroundJobFailing
        expr: |
          (
            rate(job_execution_total{status="failed"}[10m])
            / (rate(job_execution_total{status="success"}[10m]) + rate(job_execution_total{status="failed"}[10m]))
          ) > 0.05
          and
          (
            (rate(job_execution_total{status="success"}[10m]) + rate(job_execution_total{status="failed"}[10m])) > 0
          )
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Background job {{ $labels.job_name }} is failing"
          description: "Job {{ $labels.job_name }} has a failure rate of {{ $value | humanizePercentage }} (threshold: 5% over the last 10 minutes)."
          runbook: "docs/operations/runbooks/background-jobs.md#job-failures"

      # Critical Job Failure Rate
      - alert: BackgroundJobCriticalFailureRate
        expr: |
          (
            rate(job_execution_total{status="failed"}[5m])
            / (rate(job_execution_total{status="success"}[5m]) + rate(job_execution_total{status="failed"}[5m]))
          ) > 0.5
          and
          (
            (rate(job_execution_total{status="success"}[5m]) + rate(job_execution_total{status="failed"}[5m])) > 0
          )
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Critical failure rate for job {{ $labels.job_name }}"
          description: "Job {{ $labels.job_name }} has a failure rate of {{ $value | humanizePercentage }} (threshold: 50%)."
          runbook: "docs/operations/runbooks/background-jobs.md#critical-failure-rate"

      # Job Not Running (Stale)
      # Note: 2-hour threshold works for most jobs (hot_score: 5m, trending_score: 60m, clip_sync: 15m)
      # but may not detect failures in infrequent jobs (reputation_tasks: 6h). Adjust threshold or
      # create job-specific alerts for jobs with intervals > 2 hours.
      - alert: BackgroundJobNotRunning
        expr: |
          (time() - job_last_success_timestamp_seconds) > 7200
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "Background job {{ $labels.job_name }} has not run successfully"
          description: "Job {{ $labels.job_name }} has not completed successfully for {{ $value | humanizeDuration }}."
          runbook: "docs/operations/runbooks/background-jobs.md#job-not-running"

      # Critical Job Stale (24 hours)
      - alert: BackgroundJobCriticallyStale
        expr: |
          (time() - job_last_success_timestamp_seconds) > 86400
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Background job {{ $labels.job_name }} critically stale"
          description: "Job {{ $labels.job_name }} has not completed successfully for {{ $value | humanizeDuration }} (>24 hours)."
          runbook: "docs/operations/runbooks/background-jobs.md#job-critically-stale"

      # High Job Duration
      - alert: BackgroundJobHighDuration
        expr: |
          histogram_quantile(0.95,
            sum(rate(job_execution_duration_seconds_bucket[10m])) by (job_name, le)
          ) > 300
        for: 15m
        labels:
          severity: warning
        annotations:
          summary: "Background job {{ $labels.job_name }} running slowly"
          description: "P95 duration for job {{ $labels.job_name }} is {{ $value }}s (threshold: 300s)."
          runbook: "docs/operations/runbooks/background-jobs.md#high-duration"

      # Critical Job Duration
      - alert: BackgroundJobCriticalDuration
        expr: |
          histogram_quantile(0.95,
            sum(rate(job_execution_duration_seconds_bucket[10m])) by (job_name, le)
          ) > 600
        for: 10m
        labels:
          severity: critical
        annotations:
          summary: "Background job {{ $labels.job_name }} critically slow"
          description: "P95 duration for job {{ $labels.job_name }} is {{ $value }}s (threshold: 600s / 10 minutes)."
          runbook: "docs/operations/runbooks/background-jobs.md#critical-duration"

      # Job Queue Growing
      - alert: BackgroundJobQueueGrowing
        expr: |
          job_queue_size > 100
          and (job_queue_size / ((job_queue_size offset 10m) or vector(0))) > 1.5
        for: 15m
        labels:
          severity: warning
        annotations:
          summary: "Job queue for {{ $labels.job_name }} is growing"
          description: "Queue size for job {{ $labels.job_name }} is {{ $value }} and growing by >50% over 10m."
          runbook: "docs/operations/runbooks/background-jobs.md#queue-growing"

      # Critical Job Queue Size
      - alert: BackgroundJobCriticalQueueSize
        expr: |
          job_queue_size > 1000
        for: 10m
        labels:
          severity: critical
        annotations:
          summary: "Critical queue size for {{ $labels.job_name }}"
          description: "Queue size for job {{ $labels.job_name }} is {{ $value }} (threshold: 1000)."
          runbook: "docs/operations/runbooks/background-jobs.md#critical-queue-size"

      # High Item Processing Failure Rate
      - alert: BackgroundJobHighItemFailureRate
        expr: |
          (
            rate(job_items_processed_total{status="failed"}[10m])
            / clamp_min(
              rate(job_items_processed_total{status="success"}[10m]) + rate(job_items_processed_total{status="failed"}[10m]),
              0.01
            )
          ) > 0.2
          and (
            rate(job_items_processed_total{status="success"}[10m]) + rate(job_items_processed_total{status="failed"}[10m])
          ) > 0
        for: 15m
        labels:
          severity: warning
        annotations:
          summary: "High item failure rate for job {{ $labels.job_name }}"
          description: "Job {{ $labels.job_name }} has {{ $value | humanizePercentage }} item failure rate (threshold: 20%)."
          runbook: "docs/operations/runbooks/background-jobs.md#high-item-failure-rate"

  # CDN Failover Alerts
  # Monitors CDN health and failover to origin
  - name: cdn_failover_alerts
    interval: 30s
    rules:
      # CDN Failover Rate High
      - alert: CDNFailoverRateHigh
        expr: rate(cdn_failover_total[5m]) > 5
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "CDN failover rate is high"
          description: "CDN is experiencing {{ $value | humanize }} failovers/sec for 5 minutes. Assets are being served from origin."
          runbook: "docs/operations/CDN_FAILOVER_RUNBOOK.md#cdn-outage-detected"

      # CDN Failover Rate Critical
      - alert: CDNFailoverRateCritical
        expr: rate(cdn_failover_total[5m]) > 20
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "CDN failover rate is critical"
          description: "CDN is experiencing {{ $value | humanize }} failovers/sec - immediate action required. Origin server may be overloaded."
          runbook: "docs/operations/CDN_FAILOVER_RUNBOOK.md#cdn-outage-detected"

      # CDN Failover Latency High
      - alert: CDNFailoverLatencyHigh
        expr: |
          histogram_quantile(0.95,
            sum by (le) (rate(cdn_failover_duration_ms_bucket[5m]))
          ) > 500
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "CDN failover latency is high"
          description: "P95 failover latency is {{ $value | humanize }}ms (threshold: 500ms). Users may experience slow asset loading."
          runbook: "docs/operations/CDN_FAILOVER_RUNBOOK.md#cdn-outage-detected"

      # CDN Timeout Errors
      - alert: CDNTimeoutErrors
        expr: rate(cdn_failover_total{reason="timeout"}[5m]) > 2
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High rate of CDN timeout errors"
          description: "CDN is timing out at {{ $value | humanize }} req/sec. May indicate CDN network issues."
          runbook: "docs/operations/CDN_FAILOVER_RUNBOOK.md#cdn-outage-detected"

      # Origin Server High Load (due to CDN failure)
      - alert: OriginServerHighLoad
        expr: |
          (
            rate(http_requests_total{source="origin"}[5m])
            / (rate(http_requests_total{source="cdn"}[5m]) + rate(http_requests_total{source="origin"}[5m]))
          ) > 0.5
          and rate(cdn_failover_total[5m]) > 5
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "Origin server handling high traffic due to CDN failure"
          description: "Origin is serving {{ $value | humanizePercentage }} of traffic (threshold: 50%). Consider scaling if sustained."
          runbook: "docs/operations/CDN_FAILOVER_RUNBOOK.md#if-origin-server-overloaded"

      # HLS Segment Delivery Failing
      - alert: HLSSegmentDeliveryFailing
        expr: |
          (
            rate(http_requests_total{path=~".*\\.ts",status=~"5.."}[5m])
            / rate(http_requests_total{path=~".*\\.ts"}[5m])
          ) > 0.05
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "High error rate for HLS segment delivery"
          description: "HLS segments failing at {{ $value | humanizePercentage }} (threshold: 5%). Video playback is degraded."
          runbook: "docs/operations/CDN_FAILOVER_RUNBOOK.md#cdn-outage-detected"

      # HLS Playlist Delivery Failing
      - alert: HLSPlaylistDeliveryFailing
        expr: |
          (
            rate(http_requests_total{path=~".*\\.m3u8",status=~"5.."}[5m])
            / rate(http_requests_total{path=~".*\\.m3u8"}[5m])
          ) > 0.05
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "High error rate for HLS playlist delivery"
          description: "HLS playlists failing at {{ $value | humanizePercentage }} (threshold: 5%). Video playback initiation is failing."
          runbook: "docs/operations/CDN_FAILOVER_RUNBOOK.md#cdn-outage-detected"

  # Backup & Recovery Alerts
  # Related to Roadmap 5.0 Phase 5.4 - Automated Backup & Recovery
  # Issue: subculture-collective/clipper#863
  - name: clipper_backup_alerts
    interval: 60s
    rules:
      # Backup Job Failed
      - alert: BackupJobFailed
        expr: |
          (time() - postgres_backup_timestamp) > 86400
        for: 1h
        labels:
          severity: critical
          category: backup
        annotations:
          summary: "PostgreSQL backup has not succeeded in 24 hours"
          description: "Last successful backup was {{ $value | humanizeDuration }} ago. Daily backups are failing."
          runbook: "docs/operations/backup-recovery-runbook.md#backup-failures"
      
      # Backup Job Missing
      - alert: BackupJobNotRunning
        expr: |
          absent(postgres_backup_timestamp)
        for: 2h
        labels:
          severity: critical
          category: backup
        annotations:
          summary: "PostgreSQL backup job has never run"
          description: "No backup metrics found. Backup CronJob may not be configured or running."
          runbook: "docs/operations/backup-recovery-runbook.md#backup-not-running"
      
      # WAL Archiving Lag
      - alert: WALArchivingLag
        expr: |
          pg_stat_archiver_last_archived_time < (time() - 900)
        for: 15m
        labels:
          severity: warning
          category: backup
        annotations:
          summary: "PostgreSQL WAL archiving is lagging"
          description: "Last WAL archive was {{ $value | humanizeDuration }} ago. PITR may be affected."
          runbook: "docs/operations/backup-recovery-runbook.md#wal-archiving-lag"
      
      # WAL Archiving Failed
      - alert: WALArchivingFailed
        expr: |
          rate(pg_stat_archiver_failed_count[5m]) > 0
        for: 10m
        labels:
          severity: critical
          category: backup
        annotations:
          summary: "PostgreSQL WAL archiving is failing"
          description: "WAL archiving has {{ $value }} failures in the last 5 minutes. PITR is at risk."
          runbook: "docs/operations/backup-recovery-runbook.md#wal-archiving-failures"
      
      # Restore Test Failed
      - alert: RestoreTestFailed
        expr: |
          postgres_restore_test_success == 0
        for: 5m
        labels:
          severity: critical
          category: backup
        annotations:
          summary: "Monthly restore test failed"
          description: "The last restore test did not complete successfully. Backup integrity may be compromised."
          runbook: "docs/operations/backup-recovery-runbook.md#restore-test-failures"
      
      # Restore Test Not Run
      - alert: RestoreTestNotRun
        expr: |
          (time() - postgres_restore_test_timestamp) > 2678400
        for: 1h
        labels:
          severity: warning
          category: backup
        annotations:
          summary: "Restore test has not run in 31 days"
          description: "Last restore test was {{ $value | humanizeDuration }} ago. Monthly testing is required."
          runbook: "docs/operations/backup-recovery-runbook.md#restore-test-overdue"
      
      # RTO Target Exceeded
      - alert: RestoreRTOExceeded
        expr: |
          postgres_restore_test_duration_seconds > 3600
        for: 5m
        labels:
          severity: warning
          category: backup
        annotations:
          summary: "Restore test exceeded RTO target"
          description: "Last restore took {{ $value | humanizeDuration }} (target: < 1 hour). RTO SLA may not be achievable."
          runbook: "docs/operations/backup-recovery-runbook.md#rto-exceeded"
      
      # Backup Storage Low
      - alert: BackupStorageLow
        expr: |
          (backup_storage_used_bytes / backup_storage_total_bytes) > 0.85
        for: 30m
        labels:
          severity: warning
          category: backup
        annotations:
          summary: "Backup storage capacity low"
          description: "Backup storage is {{ $value | humanizePercentage }} full. Increase capacity or adjust retention."
          runbook: "docs/operations/backup-recovery-runbook.md#storage-capacity"
      
      # Volume Snapshot Failed
      - alert: VolumeSnapshotFailed
        expr: |
          kube_volumesnapshot_status_condition{condition="Ready",status="False"} == 1
        for: 15m
        labels:
          severity: warning
          category: backup
        annotations:
          summary: "Volume snapshot {{ $labels.volumesnapshot }} is not ready"
          description: "Volume snapshot {{ $labels.volumesnapshot }} failed to become ready."
          runbook: "docs/operations/backup-recovery-runbook.md#snapshot-failures"
      
      # No Recent Snapshots
      - alert: NoRecentVolumeSnapshot
        expr: |
          (time() - max(kube_volumesnapshot_created_timestamp{namespace="clipper-production"})) > 172800
        for: 1h
        labels:
          severity: warning
          category: backup
        annotations:
          summary: "No volume snapshots created in 48 hours"
          description: "Last volume snapshot was {{ $value | humanizeDuration }} ago. Daily snapshots may not be running."
          runbook: "docs/operations/backup-recovery-runbook.md#snapshot-not-running"

  # HPA Scaling Alerts
  # Monitors Horizontal Pod Autoscaler behavior and scaling events
  - name: hpa_scaling_alerts
    interval: 30s
    rules:
      # HPA at Maximum Replicas
      - alert: HPAMaxedOut
        expr: |
          kube_horizontalpodautoscaler_status_current_replicas{horizontalpodautoscaler=~"clipper-backend|clipper-frontend"}
          >= kube_horizontalpodautoscaler_spec_max_replicas{horizontalpodautoscaler=~"clipper-backend|clipper-frontend"}
        for: 15m
        labels:
          severity: warning
        annotations:
          summary: "HPA {{ $labels.horizontalpodautoscaler }} at maximum replicas"
          description: "HPA {{ $labels.horizontalpodautoscaler }} has been at max replicas ({{ $value }}) for 15 minutes. Consider increasing max replicas or investigating load."
          runbook: "docs/operations/runbooks/hpa-scaling.md#hpa-maxed-out"

      # HPA Unable to Scale
      - alert: HPAUnableToScale
        expr: |
          kube_horizontalpodautoscaler_status_condition{condition="ScalingLimited",status="true",horizontalpodautoscaler=~"clipper-backend|clipper-frontend"} == 1
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "HPA {{ $labels.horizontalpodautoscaler }} unable to scale"
          description: "HPA {{ $labels.horizontalpodautoscaler }} is unable to scale due to limits. Check resource constraints and pod quotas."
          runbook: "docs/operations/runbooks/hpa-scaling.md#unable-to-scale"

      # HPA Metrics Unavailable
      - alert: HPAMetricsUnavailable
        expr: |
          kube_horizontalpodautoscaler_status_condition{condition="ScalingActive",status="false",horizontalpodautoscaler=~"clipper-backend|clipper-frontend"} == 1
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "HPA {{ $labels.horizontalpodautoscaler }} metrics unavailable"
          description: "HPA {{ $labels.horizontalpodautoscaler }} cannot obtain metrics. Check metrics-server and Prometheus adapter health."
          runbook: "docs/operations/runbooks/hpa-scaling.md#metrics-unavailable"

      # HPA Frequent Scaling
      - alert: HPAFrequentScaling
        expr: |
          abs(
            delta(kube_horizontalpodautoscaler_status_desired_replicas{horizontalpodautoscaler=~"clipper-backend|clipper-frontend"}[10m])
          ) > 4
        for: 30m
        labels:
          severity: info
        annotations:
          summary: "HPA {{ $labels.horizontalpodautoscaler }} scaling frequently"
          description: "HPA {{ $labels.horizontalpodautoscaler }} has changed desired replicas by more than 4 in the last 10 minutes. This may indicate oscillating load or improper thresholds."
          runbook: "docs/operations/runbooks/hpa-scaling.md#frequent-scaling"

      # HPA Desired Replicas Mismatch
      - alert: HPADesiredReplicasMismatch
        expr: |
          abs(
            kube_horizontalpodautoscaler_status_desired_replicas{horizontalpodautoscaler=~"clipper-backend|clipper-frontend"}
            - kube_horizontalpodautoscaler_status_current_replicas{horizontalpodautoscaler=~"clipper-backend|clipper-frontend"}
          ) > 2
        for: 15m
        labels:
          severity: warning
        annotations:
          summary: "HPA {{ $labels.horizontalpodautoscaler }} replica mismatch"
          description: "HPA {{ $labels.horizontalpodautoscaler }} has a mismatch between desired and current replicas for 15m. Difference: {{ $value }}. Check pod scheduling and resource availability."
          runbook: "docs/operations/runbooks/hpa-scaling.md#replica-mismatch"

      # Custom Metrics Not Available
      - alert: HPACustomMetricsNotAvailable
        expr: |
          kube_horizontalpodautoscaler_status_condition{condition="AbleToScale",status="false",horizontalpodautoscaler=~"clipper-backend|clipper-frontend"}
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Custom metrics unavailable for {{ $labels.horizontalpodautoscaler }}"
          description: "HPA {{ $labels.horizontalpodautoscaler }} is unable to scale, possibly due to missing custom metrics. Check prometheus-adapter status."
          runbook: "docs/operations/runbooks/hpa-scaling.md#custom-metrics-unavailable"

      # Metrics Server Down
      - alert: MetricsServerDown
        expr: |
          kube_horizontalpodautoscaler_status_condition{condition="ScalingActive",status="false",horizontalpodautoscaler=~"clipper-backend|clipper-frontend"}
          and
          kube_horizontalpodautoscaler_status_condition{condition="AbleToScale",status="false",horizontalpodautoscaler=~"clipper-backend|clipper-frontend"}
        for: 3m
        labels:
          severity: critical
        annotations:
          summary: "Metrics Server is down"
          description: "Metrics Server is unavailable. HPA cannot function without resource metrics."
          runbook: "docs/operations/runbooks/hpa-scaling.md#metrics-server-down"

      # High Scale-Up Events
      - alert: HPAHighScaleUpRate
        expr: |
          delta(kube_horizontalpodautoscaler_status_desired_replicas{horizontalpodautoscaler=~"clipper-backend|clipper-frontend"}[5m]) > 3
        for: 10m
        labels:
          severity: info
        annotations:
          summary: "High scale-up rate for {{ $labels.horizontalpodautoscaler }}"
          description: "HPA {{ $labels.horizontalpodautoscaler }} has scaled up by more than 3 replicas in 5m. This may indicate sustained high load."
          runbook: "docs/operations/runbooks/hpa-scaling.md#high-scale-up-rate"

      # Backend at High Utilization Before Scaling
      - alert: BackendHighUtilizationBeforeScaling
        expr: |
          (
            sum(rate(http_requests_total{job="clipper-backend"}[2m]))
            / count(kube_pod_info{pod=~"clipper-backend.*"})
          ) > 900
          and kube_horizontalpodautoscaler_status_current_replicas{horizontalpodautoscaler="clipper-backend"}
          < kube_horizontalpodautoscaler_spec_max_replicas{horizontalpodautoscaler="clipper-backend"}
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Backend pods at high RPS before scaling"
          description: "Backend pods are handling >900 RPS (target: 1000) but HPA hasn't scaled yet. Check HPA responsiveness."
          runbook: "docs/operations/runbooks/hpa-scaling.md#slow-scale-up"

      # Frontend at High Utilization Before Scaling
      - alert: FrontendHighUtilizationBeforeScaling
        expr: |
          (
            sum(rate(http_requests_total{job="clipper-frontend"}[2m]))
            / count(kube_pod_info{pod=~"clipper-frontend.*"})
          ) > 900
          and kube_horizontalpodautoscaler_status_current_replicas{horizontalpodautoscaler="clipper-frontend"}
          < kube_horizontalpodautoscaler_spec_max_replicas{horizontalpodautoscaler="clipper-frontend"}
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Frontend pods at high RPS before scaling"
          description: "Frontend pods are handling >900 RPS (target: 1000) but HPA hasn't scaled yet. Check HPA responsiveness."
          runbook: "docs/operations/runbooks/hpa-scaling.md#slow-scale-up"

  - name: pgbouncer_alerts
    interval: 30s
    rules:
      # PgBouncer connection pool exhaustion
      - alert: PgBouncerPoolExhaustion
        expr: pgbouncer_pools_sv_active{database="clipper_db"} >= 48
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "PgBouncer connection pool near exhaustion"
          description: "PgBouncer has {{ $value }} active server connections (limit: 50). Risk of connection exhaustion."
          runbook: "Check database load, verify pool configuration, consider increasing max_db_connections."

      # PgBouncer high client wait time
      - alert: PgBouncerHighWaitTime
        expr: rate(pgbouncer_stats_waiting_duration_microseconds{database="clipper_db"}[5m]) / 1000 > 50
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "PgBouncer clients experiencing high wait times"
          description: "Average client wait time is {{ $value }}ms (threshold: 50ms). Connection pool may be undersized."
          runbook: "Check pool utilization, verify query performance, consider increasing pool size."

      # PgBouncer waiting clients queue
      - alert: PgBouncerWaitingClients
        expr: pgbouncer_pools_cl_waiting{database="clipper_db"} > 10
        for: 3m
        labels:
          severity: warning
        annotations:
          summary: "PgBouncer has waiting clients in queue"
          description: "{{ $value }} clients are waiting for database connections. Pool may be undersized."
          runbook: "Check current pool size, verify database performance, consider scaling."

      # PgBouncer service down
      - alert: PgBouncerDown
        expr: up{job="pgbouncer"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "PgBouncer service is down"
          description: "PgBouncer connection pooler is not responding. Database connections may fail."
          runbook: "Check PgBouncer pod status, verify configuration, review logs."

      # PgBouncer high connection errors
      - alert: PgBouncerHighErrors
        expr: rate(pgbouncer_errors_count{database="clipper_db"}[5m]) > 1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "PgBouncer experiencing high connection errors"
          description: "{{ $value }} connection errors per second. Check database connectivity and configuration."
          runbook: "Review PgBouncer logs, verify postgres connectivity, check authentication."

      # PgBouncer pool utilization
      # NOTE: This alert uses hardcoded max pool size of 50 in the calculation
      # If you change max_db_connections in pgbouncer-config, update the divisor here
      - alert: PgBouncerHighUtilization
        expr: (pgbouncer_pools_sv_active{database="clipper_db"} / 50) * 100 > 80
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "PgBouncer pool utilization high"
          description: "Pool utilization is {{ $value }}% (threshold: 80%). Consider scaling pool size."
          runbook: "Monitor connection patterns, verify pool configuration matches load."

  # Resource Quota and Limit Alerts
  # Related to Roadmap 5.0 Phase 5.2
  # See: subculture-collective/clipper#853, subculture-collective/clipper#805
  - name: clipper_quota_alerts
    interval: 30s
    rules:
      # ResourceQuota CPU Usage
      - alert: ResourceQuotaCPUNearLimit
        expr: |
          (
            kube_resourcequota{resource="requests.cpu", type="used"}
            / kube_resourcequota{resource="requests.cpu", type="hard"}
          ) * 100 > 80
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "Namespace {{ $labels.namespace }} CPU quota near limit"
          description: "CPU quota usage is {{ $value | humanize }}% in namespace {{ $labels.namespace }}."
          runbook: "Review pod resource requests, consider scaling down or increasing quota. Related to subculture-collective/clipper#853."

      # ResourceQuota Memory Usage
      - alert: ResourceQuotaMemoryNearLimit
        expr: |
          (
            kube_resourcequota{resource="requests.memory", type="used"}
            / kube_resourcequota{resource="requests.memory", type="hard"}
          ) * 100 > 80
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "Namespace {{ $labels.namespace }} memory quota near limit"
          description: "Memory quota usage is {{ $value | humanize }}% in namespace {{ $labels.namespace }}."
          runbook: "Review pod resource requests, consider scaling down or increasing quota. Related to subculture-collective/clipper#853."

      # ResourceQuota Pod Count
      - alert: ResourceQuotaPodCountNearLimit
        expr: |
          (
            kube_resourcequota{resource="pods", type="used"}
            / kube_resourcequota{resource="pods", type="hard"}
          ) * 100 > 90
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Namespace {{ $labels.namespace }} pod count near limit"
          description: "Pod count is {{ $value | humanize }}% of quota in namespace {{ $labels.namespace }}."
          runbook: "Review pod deployments, check for stuck pods, consider increasing quota."

      # Container OOM Events
      - alert: ContainerOOMKilled
        expr: |
          increase(kube_pod_container_status_restarts_total[5m]) > 0
          and on(namespace, pod, container)
          kube_pod_container_status_last_terminated_reason{reason="OOMKilled"} == 1
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Container {{ $labels.container }} in pod {{ $labels.pod }} killed due to OOM"
          description: "Container {{ $labels.container }} in namespace {{ $labels.namespace }} was terminated due to out-of-memory. Consider increasing memory limits."
          runbook: "Review container memory usage patterns, increase memory limits or optimize application memory usage. Related to subculture-collective/clipper#853."

      # Pod Memory Approaching Limit
      - alert: PodMemoryNearLimit
        expr: |
          (
            container_memory_working_set_bytes{container!="", container!="POD"}
            / container_spec_memory_limit_bytes{container!="", container!="POD"}
          ) * 100 > 90
          and on (namespace, pod, container)
          container_spec_memory_limit_bytes{container!="", container!="POD"} > 0
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "Pod {{ $labels.pod }} container {{ $labels.container }} memory usage near limit"
          description: "Container {{ $labels.container }} is using {{ $value | humanize }}% of its memory limit in namespace {{ $labels.namespace }}."
          runbook: "Monitor for OOM kills, consider increasing memory limits. Related to subculture-collective/clipper#853."

      # Pod CPU Throttling
      - alert: PodCPUThrottling
        expr: |
          sum by (namespace, pod, container) (
            rate(container_cpu_cfs_throttled_seconds_total{container!="", container!="POD"}[5m])
          ) > 0.25
        for: 15m
        labels:
          severity: warning
        annotations:
          summary: "Pod {{ $labels.pod }} container {{ $labels.container }} experiencing CPU throttling"
          description: "Container {{ $labels.container }} is being throttled {{ $value }} seconds per second in namespace {{ $labels.namespace }}. Consider increasing CPU limits."
          runbook: "Review CPU usage patterns, adjust CPU requests/limits. High throttling may impact performance."

      # LimitRange Violations (pod events)
      # Note: This alert may have false positives as CreateContainerConfigError
      # can occur for various reasons. Check pod events for specific error details.
      - alert: LimitRangeViolationSuspected
        expr: |
          increase(kube_pod_container_status_waiting_reason{reason="CreateContainerConfigError"}[5m]) > 0
        for: 5m
        labels:
          severity: info
        annotations:
          summary: "Pod {{ $labels.pod }} has container configuration error"
          description: "Pod {{ $labels.pod }} in namespace {{ $labels.namespace }} is waiting due to CreateContainerConfigError. This may indicate LimitRange violation or other configuration issues."
          runbook: "Check pod events with 'kubectl describe pod' and verify: 1) Resource specs against LimitRange 2) Secrets/ConfigMaps exist 3) Image pull configuration."

      # Storage Quota Near Limit
      - alert: StorageQuotaNearLimit
        expr: |
          (
            kube_resourcequota{resource="requests.storage", type="used"}
            / kube_resourcequota{resource="requests.storage", type="hard"}
          ) * 100 > 85
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "Namespace {{ $labels.namespace }} storage quota near limit"
          description: "Storage quota usage is {{ $value | humanize }}% in namespace {{ $labels.namespace }}."
          runbook: "Review PVC usage, clean up unused volumes, consider increasing storage quota."

      # PVC Count Near Limit
      - alert: PVCQuotaNearLimit
        expr: |
          (
            kube_resourcequota{resource="persistentvolumeclaims", type="used"}
            / kube_resourcequota{resource="persistentvolumeclaims", type="hard"}
          ) * 100 > 90
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "Namespace {{ $labels.namespace }} PVC count near limit"
          description: "PVC count is {{ $value | humanize }}% of quota in namespace {{ $labels.namespace }}."
          runbook: "Review PersistentVolumeClaims, delete unused volumes, consider increasing quota."

  # DDoS Protection & Traffic Alerts
  # Related Issues (Roadmap 5.0 - Phase 5.4):
  # - #862 - DDoS Protection
  # - #861 - WAF Protection (related)
  # - #805 - Infrastructure Security
  - name: clipper_ddos_alerts
    interval: 30s
    rules:
      # High traffic volume (potential DDoS)
      - alert: SuspiciouslyHighTrafficVolume
        expr: |
          sum(rate(nginx_ingress_controller_requests[1m])) > 1000
        for: 2m
        labels:
          severity: warning
          category: ddos
        annotations:
          summary: "Unusually high traffic volume detected"
          description: "Request rate is {{ $value | humanize }} req/s, which is significantly above normal baseline."
          runbook: "Check traffic analytics dashboard at /d/ddos-traffic-analytics. Review top IPs. See docs/operations/ddos-protection.md for mitigation procedures."
      
      # Critical: Very high traffic (likely DDoS)
      - alert: DDoSAttackSuspected
        expr: |
          sum(rate(nginx_ingress_controller_requests[1m])) > 5000
        for: 1m
        labels:
          severity: critical
          category: ddos
        annotations:
          summary: "Potential DDoS attack in progress"
          description: "Request rate is {{ $value | humanize }} req/s, far exceeding normal capacity."
          runbook: "URGENT: Follow DDoS mitigation runbook at docs/operations/ddos-protection.md. Enable aggressive rate limiting, contact cloud provider support, review incident response plan."
      
      # Rate limit frequently triggered
      - alert: HighRateLimitHitRate
        expr: |
          sum(rate(nginx_ingress_controller_requests{status="429"}[5m])) > 50
        for: 5m
        labels:
          severity: warning
          category: ddos
        annotations:
          summary: "High rate of rate limit hits"
          description: "{{ $value | humanize }} rate limit hits per second. May indicate DDoS attempt or misconfigured client."
          runbook: "Review rate-limited IPs in dashboard, check if legitimate traffic, adjust rate limits if needed. See docs/operations/waf-protection.md."
      
      # Multiple IPs banned (coordinated attack)
      - alert: MultipleIPsBanned
        expr: |
          sum(rate(abuse_detection_ip_bans_total[5m])) > 10
        for: 5m
        labels:
          severity: critical
          category: ddos
        annotations:
          summary: "Multiple IPs banned for abuse"
          description: "{{ $value | humanize }} IPs banned in last 5 minutes. Likely coordinated DDoS attack."
          runbook: "Review banned IP list in Redis. Identify attack patterns. Enable additional edge protection. See docs/operations/ddos-protection.md#incident-response-flow."
      
      # High error rate (resource exhaustion)
      - alert: HighErrorRateDDoS
        expr: |
          (
            sum(rate(nginx_ingress_controller_requests{status=~"5.."}[5m]))
            / sum(rate(nginx_ingress_controller_requests[5m]))
          ) > 0.1
        for: 3m
        labels:
          severity: critical
          category: ddos
        annotations:
          summary: "High server error rate (potential resource exhaustion)"
          description: "{{ $value | humanizePercentage }} of requests returning 5xx errors. May indicate DDoS-induced resource exhaustion."
          runbook: "Check application health and resource utilization. Scale up if needed. Review if DDoS causing resource exhaustion."
      
      # Connection limit saturation
      - alert: ConnectionLimitSaturation
        expr: |
          nginx_ingress_controller_nginx_process_connections{state="active"} > 50000
        for: 2m
        labels:
          severity: warning
          category: ddos
        annotations:
          summary: "High number of active connections"
          description: "{{ $value | humanize }} active connections. May be approaching ingress controller limits."
          runbook: "Review connection sources in dashboard. Scale ingress controller if needed. Check for connection leak or slowloris attack."
      
      # Single IP excessive traffic
      - alert: SingleIPExcessiveTraffic
        expr: |
          topk(1, sum(rate(nginx_ingress_controller_requests[5m])) by (client_ip)) > 100
        for: 5m
        labels:
          severity: warning
          category: ddos
        annotations:
          summary: "Single IP generating excessive traffic"
          description: "IP {{ $labels.client_ip }} generating {{ $value | humanize }} req/s."
          runbook: "Investigate IP {{ $labels.client_ip }}. Check if legitimate (monitoring, load testing). Consider manual IP block if malicious."
      
      # Traffic spike anomaly (>5x baseline)
      - alert: TrafficSpikeAnomaly
        expr: |
          sum(rate(nginx_ingress_controller_requests[5m])) > 
          5 * avg_over_time(sum(rate(nginx_ingress_controller_requests[5m]))[1h:5m])
        for: 3m
        labels:
          severity: warning
          category: ddos
        annotations:
          summary: "Traffic spike detected (>5x baseline)"
          description: "Current request rate is >5x the 1-hour baseline. May indicate traffic surge or DDoS attack."
          runbook: "Check traffic analytics dashboard. Verify if legitimate traffic spike (launch, promotion). Apply rate limiting if attack."
      
      # Ingress controller CPU saturation
      - alert: IngressControllerCPUSaturation
        expr: |
          rate(container_cpu_usage_seconds_total{namespace="ingress-nginx",container="controller"}[5m]) > 0.9
        for: 5m
        labels:
          severity: critical
          category: ddos
        annotations:
          summary: "Ingress controller CPU saturated"
          description: "Ingress controller CPU usage at {{ $value | humanizePercentage }}. May indicate DDoS attack overwhelming edge layer."
          runbook: "Scale ingress controller replicas immediately. Review traffic patterns. Apply aggressive rate limiting."
      
      # Ingress controller memory pressure
      - alert: IngressControllerMemoryPressure
        expr: |
          (
            container_memory_working_set_bytes{namespace="ingress-nginx",container="controller"}
            / container_spec_memory_limit_bytes{namespace="ingress-nginx",container="controller"}
          ) > 0.85
        for: 5m
        labels:
          severity: warning
          category: ddos
        annotations:
          summary: "Ingress controller memory pressure"
          description: "Ingress controller memory usage at {{ $value | humanizePercentage }}. May be exhausted by connection tracking."
          runbook: "Monitor for OOM kills. Scale ingress controller if persistent. Check for memory leak or excessive connections."

  # Twitch Moderation Ban/Unban Alerts
  # Related Issues (Epic #1059 - Phase P2: Observability):
  # - Monitors Twitch ban/unban actions for errors, rate limits, and latency
  # - Tracks permission/scope issues, server errors, and rate limit hits
  - name: twitch_moderation_alerts
    interval: 30s
    rules:
      # High Twitch Ban Failure Rate
      - alert: TwitchBanHighFailureRate
        expr: |
          (
            sum(rate(twitch_ban_action_total{status="failed"}[5m]))
            / sum(rate(twitch_ban_action_total[5m]))
          ) > 0.1
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "High failure rate for Twitch ban/unban actions"
          description: "Twitch ban/unban failure rate is {{ $value | humanizePercentage }} (threshold: 10%)."
          runbook: "Check Twitch API status, verify OAuth scopes, review error logs for common patterns."
      
      # Critical Twitch Ban Failure Rate
      - alert: TwitchBanCriticalFailureRate
        expr: |
          (
            sum(rate(twitch_ban_action_total{status="failed"}[5m]))
            / sum(rate(twitch_ban_action_total[5m]))
          ) > 0.5
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Critical failure rate for Twitch ban/unban actions"
          description: "Twitch ban/unban failure rate is {{ $value | humanizePercentage }} (threshold: 50%). Immediate investigation required."
          runbook: "Check Twitch API status page, verify service health, review authentication configuration."
      
      # Elevated Permission/Scope Errors
      - alert: TwitchBanPermissionErrors
        expr: |
          rate(twitch_ban_permission_errors_total[5m]) > 1
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "Elevated permission/scope errors for Twitch ban actions"
          description: "{{ $value }} permission errors per second for {{ $labels.error_type }}. Users may have insufficient Twitch OAuth scopes."
          runbook: "Review OAuth scope configuration, verify users have required Twitch permissions (moderator:manage:banned_users or channel:manage:banned_users). Check documentation for scope requirements."
      
      # Twitch API Rate Limit Hits
      - alert: TwitchBanRateLimitHits
        expr: |
          rate(twitch_ban_rate_limit_hits_total[5m]) > 0.5
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Twitch ban API rate limit being hit"
          description: "{{ $value }} rate limit hits per second for {{ $labels.action }}. Actions are being throttled by Twitch."
          runbook: "Implement exponential backoff for retries, reduce request rate, or request rate limit increase from Twitch if legitimate usage."
      
      # Critical Rate Limit Hits
      - alert: TwitchBanCriticalRateLimitHits
        expr: |
          rate(twitch_ban_rate_limit_hits_total[5m]) > 2
        for: 3m
        labels:
          severity: critical
        annotations:
          summary: "Critical rate of Twitch ban API rate limit hits"
          description: "{{ $value }} rate limit hits per second for {{ $labels.action }}. High volume of throttled requests."
          runbook: "Immediate action required. Implement circuit breaker, apply request queuing, contact Twitch support if needed."
      
      # Elevated 5xx Server Errors from Twitch
      - alert: TwitchBanServerErrors
        expr: |
          rate(twitch_ban_server_errors_total[5m]) > 0.5
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Elevated 5xx errors from Twitch ban API"
          description: "{{ $value }} server errors per second (status {{ $labels.status_code }}) for {{ $labels.action }}."
          runbook: "Check Twitch API status page (https://devstatus.twitch.tv/), implement retry logic with exponential backoff."
      
      # Critical 5xx Error Rate
      - alert: TwitchBanCriticalServerErrors
        expr: |
          rate(twitch_ban_server_errors_total[5m]) > 2
        for: 3m
        labels:
          severity: critical
        annotations:
          summary: "Critical rate of 5xx errors from Twitch ban API"
          description: "{{ $value }} server errors per second (status {{ $labels.status_code }}) for {{ $labels.action }}. Twitch API may be experiencing issues."
          runbook: "Check Twitch API status page, enable circuit breaker, notify users of degraded Twitch integration."
      
      # High Latency for Ban/Unban Operations
      - alert: TwitchBanHighLatency
        expr: |
          histogram_quantile(0.95,
            sum(rate(twitch_ban_action_duration_seconds_bucket[5m])) by (action, le)
          ) > 5
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "High latency for Twitch {{ $labels.action }} operations"
          description: "P95 latency for Twitch {{ $labels.action }} is {{ $value }}s (threshold: 5s)."
          runbook: "Check Twitch API performance, verify network connectivity, review timeout configurations."
      
      # Critical Latency
      - alert: TwitchBanCriticalLatency
        expr: |
          histogram_quantile(0.95,
            sum(rate(twitch_ban_action_duration_seconds_bucket[5m])) by (action, le)
          ) > 10
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Critical latency for Twitch {{ $labels.action }} operations"
          description: "P95 latency for Twitch {{ $labels.action }} is {{ $value }}s (threshold: 10s). Immediate investigation required."
          runbook: "Check Twitch API status, verify network path, consider implementing timeout and fallback mechanisms."
      
      # High 4xx Client Error Rate (excluding rate limits handled separately)
      - alert: TwitchBan4xxErrors
        expr: |
          rate(twitch_ban_http_status_total{status_class="4xx",status_code!="429"}[5m]) > 1
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "Elevated 4xx client errors for Twitch ban API"
          description: "{{ $value }} client errors per second (status {{ $labels.status_code }}) for {{ $labels.action }}."
          runbook: "Review request validation, check for malformed requests, verify user permissions and scopes."

