# Prometheus Alert Rules for Clipper
# Place this file in /opt/clipper/monitoring/alerts.yml
#
# These alerts are based on defined SLOs:
# - Availability: 99.5% uptime
# - Latency: P95 < 100ms (list), P95 < 50ms (detail)
# - Error Rate: < 0.5%

groups:
  - name: clipper_slo_alerts
    interval: 30s
    rules:
      # SLO: Availability - 99.5% uptime
      - alert: SLOAvailabilityBreach
        expr: |
          (
            1 - (sum(rate(http_requests_total{status=~"2.."}[5m])) / sum(rate(http_requests_total[5m])))
          ) > 0.005
        for: 5m
        labels:
          severity: critical
          slo: availability
        annotations:
          summary: "Availability SLO breach"
          description: "Service availability is {{ $value | humanizePercentage }}, target is 99.5%."
          runbook: "Check service health, investigate errors, consider rollback."

      # SLO: Error Rate - < 0.5%
      - alert: SLOErrorRateBreach
        expr: |
          (
            sum(rate(http_requests_total{status=~"5.."}[5m])) / sum(rate(http_requests_total[5m]))
          ) > 0.005
        for: 5m
        labels:
          severity: critical
          slo: error_rate
        annotations:
          summary: "Error rate SLO breach"
          description: "Error rate is {{ $value | humanizePercentage }}, target is < 0.5%."
          runbook: "Check error logs, identify error patterns, apply fix or rollback."

      # SLO: Latency - P95 < 100ms for list endpoints
      - alert: SLOLatencyBreach
        expr: |
          histogram_quantile(0.95,
            sum(rate(http_request_duration_seconds_bucket{path=~"/api/v1/(clips|feed|lists).*"}[5m])) by (le)
          ) > 0.1
        for: 5m
        labels:
          severity: warning
          slo: latency
        annotations:
          summary: "Latency SLO breach for list endpoints"
          description: "P95 latency is {{ $value }}s, target is < 0.1s."
          runbook: "Check database query performance, Redis cache hit rate, system resources."

      # Error Budget: Fast burn (> 10% in 1 hour)
      - alert: ErrorBudgetFastBurn
        expr: |
          (
            1 - (sum(rate(http_requests_total{status=~"2.."}[1h])) / sum(rate(http_requests_total[1h])))
          ) > 0.1
        for: 5m
        labels:
          severity: critical
          error_budget: fast_burn
        annotations:
          summary: "Error budget fast burn detected"
          description: "Consuming > 10% error budget in 1 hour. Current availability: {{ $value | humanizePercentage }}."
          runbook: "Immediate action required. Investigate and mitigate ongoing issues."

      # Error Budget: Medium burn (> 25% in 6 hours)
      - alert: ErrorBudgetMediumBurn
        expr: |
          (
            1 - (sum(rate(http_requests_total{status=~"2.."}[6h])) / sum(rate(http_requests_total[6h])))
          ) > 0.25
        for: 15m
        labels:
          severity: warning
          error_budget: medium_burn
        annotations:
          summary: "Error budget medium burn detected"
          description: "Consuming > 25% error budget in 6 hours. Current availability: {{ $value | humanizePercentage }}."
          runbook: "Review recent changes, implement stability improvements."

  - name: clipper_service_alerts
    interval: 30s
    rules:
      # Service Down
      - alert: ServiceDown
        expr: up == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Service {{ $labels.job }} is down"
          description: "{{ $labels.job }} has been down for more than 1 minute."

      # High Error Rate
      - alert: HighErrorRate
        expr: |
          (
            sum(rate(http_requests_total{status=~"5.."}[5m])) by (job)
            /
            sum(rate(http_requests_total[5m])) by (job)
          ) > 0.05
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High error rate on {{ $labels.job }}"
          description: "{{ $labels.job }} has an error rate of {{ $value | humanizePercentage }} over the last 5 minutes."

      # Critical Error Rate (P1)
      - alert: CriticalErrorRate
        expr: |
          (
            sum(rate(http_requests_total{status=~"5.."}[5m])) by (job)
            /
            sum(rate(http_requests_total[5m])) by (job)
          ) > 0.01
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Critical error rate on {{ $labels.job }}"
          description: "{{ $labels.job }} has a critical error rate of {{ $value | humanizePercentage }} (threshold: > 1%)."

      # High Response Time
      - alert: HighResponseTime
        expr: |
          histogram_quantile(0.95,
            sum(rate(http_request_duration_seconds_bucket[5m])) by (job, le)
          ) > 0.5
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High response time on {{ $labels.job }}"
          description: "{{ $labels.job }} 95th percentile response time is {{ $value }}s."

  - name: clipper_resource_alerts
    interval: 30s
    rules:
      # High CPU Usage
      - alert: HighCPUUsage
        expr: |
          100 - (avg by (instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High CPU usage on {{ $labels.instance }}"
          description: "CPU usage is {{ $value | humanize }}% on {{ $labels.instance }}."

      # High Memory Usage
      - alert: HighMemoryUsage
        expr: |
          (
            node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes
          ) / node_memory_MemTotal_bytes * 100 > 80
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High memory usage on {{ $labels.instance }}"
          description: "Memory usage is {{ $value | humanize }}% on {{ $labels.instance }}."

      # Low Disk Space
      - alert: LowDiskSpace
        expr: |
          (
            node_filesystem_avail_bytes{mountpoint="/"}
            / node_filesystem_size_bytes{mountpoint="/"}
          ) * 100 < 20
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Low disk space on {{ $labels.instance }}"
          description: "Disk space is {{ $value | humanize }}% free on {{ $labels.instance }}."

      # Critical Disk Space
      - alert: CriticalDiskSpace
        expr: |
          (
            node_filesystem_avail_bytes{mountpoint="/"}
            / node_filesystem_size_bytes{mountpoint="/"}
          ) * 100 < 10
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Critical disk space on {{ $labels.instance }}"
          description: "Disk space is {{ $value | humanize }}% free on {{ $labels.instance }}."

  - name: clipper_database_alerts
    interval: 30s
    rules:
      # Database Connection Issues
      - alert: DatabaseConnectionIssues
        expr: |
          sum(pg_stat_database_numbackends) by (datname) > 80
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High number of database connections"
          description: "Database {{ $labels.datname }} has {{ $value }} active connections."

      # Database Down
      - alert: DatabaseDown
        expr: pg_up == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "PostgreSQL is down"
          description: "PostgreSQL database is not responding."

      # Slow Queries
      - alert: SlowQueries
        expr: |
          rate(pg_stat_activity_max_tx_duration[5m]) > 30
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Slow database queries detected"
          description: "Database has queries taking more than 30 seconds."

  - name: clipper_redis_alerts
    interval: 30s
    rules:
      # Redis Down
      - alert: RedisDown
        expr: redis_up == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Redis is down"
          description: "Redis cache is not responding."

      # High Redis Memory Usage
      - alert: HighRedisMemoryUsage
        expr: |
          (redis_memory_used_bytes / redis_memory_max_bytes) * 100 > 80
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High Redis memory usage"
          description: "Redis is using {{ $value | humanize }}% of allocated memory."

      # Low Cache Hit Rate
      - alert: LowCacheHitRate
        expr: |
          (
            rate(redis_keyspace_hits_total[5m]) + rate(redis_keyspace_misses_total[5m])
          ) > 0.5
          and (
            rate(redis_keyspace_hits_total[5m])
            / (rate(redis_keyspace_hits_total[5m]) + rate(redis_keyspace_misses_total[5m]))
          ) < 0.8
        for: 30m
        labels:
          severity: info
        annotations:
          summary: "Low Redis cache hit rate"
          description: "Redis cache hit rate is {{ $value | humanizePercentage }} (only alerts when >0.5 req/s traffic)."

  - name: clipper_ssl_alerts
    interval: 1d
    rules:
      # SSL Certificate Expiring Soon
      - alert: SSLCertificateExpiringSoon
        expr: |
          (ssl_certificate_expiry_seconds - time()) / 86400 < 30
        for: 1h
        labels:
          severity: warning
        annotations:
          summary: "SSL certificate expiring soon"
          description: "SSL certificate for {{ $labels.domain }} expires in {{ $value }} days."

      # SSL Certificate Expired
      - alert: SSLCertificateExpired
        expr: |
          (ssl_certificate_expiry_seconds - time()) < 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "SSL certificate expired"
          description: "SSL certificate for {{ $labels.domain }} has expired."

  - name: clipper_search_alerts
    interval: 30s
    rules:
      # Semantic Search High Latency
      - alert: SemanticSearchHighLatency
        expr: |
          histogram_quantile(0.95, sum(rate(search_query_duration_ms_bucket{search_type="hybrid"}[5m])) by (le)) > 200
        for: 5m
        labels:
          severity: warning
          slo: search_latency
        annotations:
          summary: "Semantic search P95 latency exceeds 200ms"
          description: "Hybrid search P95 latency is {{ $value }}ms, target is < 200ms."
          runbook: "docs/operations/playbooks/search-incidents.md#high-latency"

      # Search Critical Latency
      - alert: SemanticSearchCriticalLatency
        expr: |
          histogram_quantile(0.95, sum(rate(search_query_duration_ms_bucket{search_type="hybrid"}[5m])) by (le)) > 500
        for: 3m
        labels:
          severity: critical
          slo: search_latency
        annotations:
          summary: "Semantic search P95 latency critical (>500ms)"
          description: "Hybrid search P95 latency is {{ $value }}ms. Immediate action required."
          runbook: "docs/operations/playbooks/search-incidents.md#critical-latency"

      # Embedding Generation Failures
      - alert: EmbeddingGenerationFailing
        expr: |
          rate(embedding_generation_errors_total[5m]) > 0.1
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Embedding generation failing"
          description: "Embedding generation error rate is {{ $value }}/sec. This affects semantic search quality."
          runbook: "docs/operations/playbooks/search-incidents.md#embedding-failures"

      # Low Embedding Coverage
      - alert: LowEmbeddingCoverage
        expr: |
          (clips_with_embeddings + clips_without_embeddings) > 0 
          and clips_without_embeddings / (clips_with_embeddings + clips_without_embeddings) > 0.1
        for: 30m
        labels:
          severity: warning
        annotations:
          summary: "Low embedding coverage"
          description: "{{ $value | humanizePercentage }} of clips are missing embeddings (threshold: 10%)."
          runbook: "docs/operations/playbooks/search-incidents.md#low-coverage"

      # Embedding Cache Low Hit Rate
      - alert: EmbeddingCacheLowHitRate
        expr: |
          (rate(embedding_cache_hits_total[5m]) + rate(embedding_cache_misses_total[5m])) > 0
          and (rate(embedding_cache_hits_total[5m]) / (rate(embedding_cache_hits_total[5m]) + rate(embedding_cache_misses_total[5m]))) < 0.5
        for: 15m
        labels:
          severity: warning
        annotations:
          summary: "Embedding cache hit rate below 50%"
          description: "Embedding cache hit rate is {{ $value | humanizePercentage }}. This increases API costs and latency."
          runbook: "docs/operations/playbooks/search-incidents.md#cache-issues"

      # High Zero Result Rate
      - alert: HighZeroResultRate
        expr: |
          rate(search_queries_total[5m]) > 0
          and rate(search_zero_results_total[5m]) / rate(search_queries_total[5m]) > 0.1
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "High zero result rate in search"
          description: "{{ $value | humanizePercentage }} of searches return no results."
          runbook: "docs/operations/playbooks/search-incidents.md#zero-results"

      # Search Fallback Activated
      - alert: SearchFallbackActivated
        expr: |
          rate(search_fallback_total[5m]) > 0.5
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Search falling back to BM25 frequently"
          description: "Search is falling back to BM25-only at {{ $value }}/sec due to {{ $labels.reason }}."
          runbook: "docs/operations/playbooks/search-incidents.md#fallback-issues"

      # Indexing Job Failures
      - alert: IndexingJobFailing
        expr: |
          increase(indexing_jobs_total{status="failed"}[1h]) > 3
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Indexing jobs failing repeatedly"
          description: "{{ $value }} indexing jobs failed in the last hour."
          runbook: "docs/operations/playbooks/search-incidents.md#indexing-failures"

      # Vector Search Slow
      - alert: VectorSearchSlow
        expr: |
          histogram_quantile(0.95, sum(rate(vector_search_duration_ms_bucket[5m])) by (le)) > 100
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Vector re-ranking is slow"
          description: "Vector search P95 latency is {{ $value }}ms, target is < 100ms."
          runbook: "docs/operations/playbooks/search-incidents.md#vector-search-slow"

      # BM25 Search Slow
      - alert: BM25SearchSlow
        expr: |
          histogram_quantile(0.95, sum(rate(bm25_search_duration_ms_bucket[5m])) by (le)) > 50
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "BM25 candidate search is slow"
          description: "BM25 search P95 latency is {{ $value }}ms, target is < 50ms."
          runbook: "docs/operations/playbooks/search-incidents.md#bm25-search-slow"

  - name: clipper_webhook_alerts
    interval: 30s
    rules:
      # High Webhook Failure Rate
      - alert: HighWebhookFailureRate
        expr: |
          (
            sum(rate(webhook_delivery_total{status="failed"}[5m]))
            /
            sum(rate(webhook_delivery_total[5m]))
          ) > 0.1
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "High webhook delivery failure rate"
          description: "Webhook delivery failure rate is {{ $value | humanizePercentage }}, target is < 10%."
          runbook: "docs/backend/webhooks.md#troubleshooting"

      # Critical Webhook Failure Rate
      - alert: CriticalWebhookFailureRate
        expr: |
          (
            sum(rate(webhook_delivery_total{status="failed"}[5m]))
            /
            sum(rate(webhook_delivery_total[5m]))
          ) > 0.5
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Critical webhook delivery failure rate"
          description: "Webhook delivery failure rate is {{ $value | humanizePercentage }} (threshold: 50%)."
          runbook: "docs/backend/webhooks.md#troubleshooting"

      # Large Webhook Retry Queue
      - alert: LargeWebhookRetryQueue
        expr: webhook_retry_queue_size > 100
        for: 15m
        labels:
          severity: warning
        annotations:
          summary: "Large webhook retry queue"
          description: "Webhook retry queue has {{ $value }} items pending."
          runbook: "docs/backend/webhook-retry.md#high-retry-queue-size"

      # Critical Webhook Retry Queue
      - alert: CriticalWebhookRetryQueue
        expr: webhook_retry_queue_size > 500
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Critical webhook retry queue size"
          description: "Webhook retry queue has {{ $value }} items pending. Immediate action required."
          runbook: "docs/backend/webhook-retry.md#high-retry-queue-size"

      # Webhook Dead-Letter Queue Items
      - alert: WebhookDeadLetterQueueItems
        expr: webhook_dead_letter_queue_size > 10
        for: 30m
        labels:
          severity: warning
        annotations:
          summary: "Webhooks in dead-letter queue"
          description: "{{ $value }} webhook deliveries have failed permanently and are in the dead-letter queue."
          runbook: "docs/backend/webhook-retry.md#events-in-dlq"

      # Critical Webhook Dead-Letter Queue
      - alert: CriticalWebhookDeadLetterQueue
        expr: webhook_dead_letter_queue_size > 50
        for: 15m
        labels:
          severity: critical
        annotations:
          summary: "Critical number of webhooks in dead-letter queue"
          description: "{{ $value }} webhook deliveries have failed permanently. Investigation required."
          runbook: "docs/backend/webhook-retry.md#events-in-dlq"

      # High Webhook Delivery Latency
      - alert: HighWebhookDeliveryLatency
        expr: |
          histogram_quantile(0.95,
            sum(rate(webhook_delivery_duration_seconds_bucket{status="success"}[5m])) by (le)
          ) > 5
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "High webhook delivery latency"
          description: "P95 webhook delivery latency is {{ $value }}s, target is < 5s."
          runbook: "docs/backend/webhooks.md#troubleshooting"

      # No Active Webhook Subscriptions (informational)
      - alert: NoActiveWebhookSubscriptions
        expr: webhook_subscriptions_active == 0
        for: 1h
        labels:
          severity: info
        annotations:
          summary: "No active webhook subscriptions"
          description: "There are currently no active webhook subscriptions configured."
          runbook: "docs/backend/webhooks.md"

  - name: clipper_logging_alerts
    interval: 30s
    rules:
      # High Error Log Rate
      - alert: HighErrorLogRate
        expr: |
          sum(rate({level="error"}[5m])) by (service) > 10
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High error log rate in {{ $labels.service }}"
          description: "{{ $labels.service }} is generating {{ $value }} error logs per second."
          runbook: "Check logs for error patterns and investigate root cause."

      # Critical Error Spike
      - alert: CriticalErrorSpike
        expr: |
          sum(rate({level="error"}[5m])) by (service) > 50
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Critical error spike in {{ $labels.service }}"
          description: "{{ $labels.service }} is experiencing {{ $value }} errors/sec. Immediate action required."
          runbook: "Investigate error patterns, consider rollback if recent deployment."

      # Security Event: Failed Authentication
      - alert: FailedAuthenticationSpike
        expr: |
          sum(rate({message=~".*authentication failed.*|.*login failed.*|.*unauthorized.*"}[5m])) by (service) > 5
        for: 3m
        labels:
          severity: warning
          security: true
        annotations:
          summary: "Failed authentication spike in {{ $labels.service }}"
          description: "{{ $labels.service }} has {{ $value }} failed auth attempts per second."
          runbook: "Check for brute force attacks, review IP addresses, consider rate limiting."

      # Security Event: SQL Injection Attempt
      - alert: SQLInjectionAttempt
        expr: |
          count_over_time({level="error",message=~".*(?i)(union.*select|insert.*into|drop.*table|delete.*from).*"}[5m]) > 0
        for: 1m
        labels:
          severity: critical
          security: true
        annotations:
          summary: "Potential SQL injection attempt detected"
          description: "{{ $value }} potential SQL injection patterns detected in logs."
          runbook: "Review logs immediately, block suspicious IPs, verify input validation."

      # Security Event: Suspicious Activity
      - alert: SuspiciousSecurityEvent
        expr: |
          sum(rate({message=~".*(?i)(security|breach|intrusion|malicious|exploit).*",level="warn"}[5m])) by (service) > 1
        for: 2m
        labels:
          severity: critical
          security: true
        annotations:
          summary: "Suspicious security events in {{ $labels.service }}"
          description: "{{ $labels.service }} has {{ $value }} security-related warnings per second."
          runbook: "Investigate security logs, check for breach indicators, notify security team."

      # Application Panic/Crash
      - alert: ApplicationPanic
        expr: |
          count_over_time({message=~".*(?i)(panic|crash|fatal).*"}[5m]) > 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Application panic detected in {{ $labels.service }}"
          description: "{{ $value }} panic/crash events detected. Application may be unstable."
          runbook: "Check stack traces, review recent deployments, consider rollback."

      # No Logs Received
      - alert: NoLogsReceived
        expr: |
          absent_over_time(({job="backend"} or {job="frontend"})[10m])
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "No logs received from {{ $labels.job }}"
          description: "No logs have been received from {{ $labels.job }} for 10 minutes."
          runbook: "Check if application is running, verify log shipping configuration."

      # High Log Volume
      - alert: HighLogVolume
        expr: |
          sum(rate({job=~"backend|frontend"}[5m])) by (service) > 1000
        for: 10m
        labels:
          severity: info
        annotations:
          summary: "High log volume from {{ $labels.service }}"
          description: "{{ $labels.service }} is generating {{ $value }} logs per second."
          runbook: "Review if this is expected behavior or indicates an issue."

      # Repeated Error Pattern (optimized query)
      - alert: RepeatedErrorPattern
        expr: |
          count(count_over_time({level="error",service=~".+"} [1h])) by (service) > 100
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Repeated error pattern detected in {{ $labels.service }}"
          description: "{{ $labels.service }} has {{ $value }} error occurrences in the last hour."
          runbook: "Investigate root cause of repeated errors in the service."

      # Database Connection Errors in Logs
      - alert: DatabaseConnectionErrors
        expr: |
          sum(rate({message=~".*(?i)(database.*connection.*failed|postgres.*error|connection.*refused).*",level="error"}[5m])) by (service) > 1
        for: 3m
        labels:
          severity: critical
        annotations:
          summary: "Database connection errors in {{ $labels.service }}"
          description: "{{ $labels.service }} has {{ $value }} database connection errors per second."
          runbook: "Check database health, verify connection pool configuration."

      # Redis Connection Errors in Logs
      - alert: RedisConnectionErrors
        expr: |
          sum(rate({message=~".*(?i)(redis.*connection.*failed|redis.*error|redis.*timeout).*",level="error"}[5m])) by (service) > 1
        for: 3m
        labels:
          severity: warning
        annotations:
          summary: "Redis connection errors in {{ $labels.service }}"
          description: "{{ $labels.service }} has {{ $value }} Redis connection errors per second."
          runbook: "Check Redis health, verify connection configuration."
