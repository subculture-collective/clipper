# Prometheus Alert Rules for Clipper
# Place this file in /opt/clipper/monitoring/alerts.yml
#
# These alerts are based on defined SLOs:
# - Availability: 99.5% uptime
# - Latency: P95 < 100ms (list), P95 < 50ms (detail)
# - Error Rate: < 0.5%

groups:
  - name: clipper_slo_alerts
    interval: 30s
    rules:
      # SLO: Availability - 99.5% uptime
      - alert: SLOAvailabilityBreach
        expr: |
          (
            1 - (sum(rate(http_requests_total{status=~"2.."}[5m])) / sum(rate(http_requests_total[5m])))
          ) > 0.005
        for: 5m
        labels:
          severity: critical
          slo: availability
        annotations:
          summary: "Availability SLO breach"
          description: "Service availability is {{ $value | humanizePercentage }}, target is 99.5%."
          runbook: "Check service health, investigate errors, consider rollback."

      # SLO: Error Rate - < 0.5%
      - alert: SLOErrorRateBreach
        expr: |
          (
            sum(rate(http_requests_total{status=~"5.."}[5m])) / sum(rate(http_requests_total[5m]))
          ) > 0.005
        for: 5m
        labels:
          severity: critical
          slo: error_rate
        annotations:
          summary: "Error rate SLO breach"
          description: "Error rate is {{ $value | humanizePercentage }}, target is < 0.5%."
          runbook: "Check error logs, identify error patterns, apply fix or rollback."

      # SLO: Latency - P95 < 100ms for list endpoints
      - alert: SLOLatencyBreach
        expr: |
          histogram_quantile(0.95,
            sum(rate(http_request_duration_seconds_bucket{path=~"/api/v1/(clips|feed|lists).*"}[5m])) by (le)
          ) > 0.1
        for: 5m
        labels:
          severity: warning
          slo: latency
        annotations:
          summary: "Latency SLO breach for list endpoints"
          description: "P95 latency is {{ $value }}s, target is < 0.1s."
          runbook: "Check database query performance, Redis cache hit rate, system resources."

      # Error Budget: Fast burn (> 10% in 1 hour)
      - alert: ErrorBudgetFastBurn
        expr: |
          (
            1 - (sum(rate(http_requests_total{status=~"2.."}[1h])) / sum(rate(http_requests_total[1h])))
          ) > 0.1
        for: 5m
        labels:
          severity: critical
          error_budget: fast_burn
        annotations:
          summary: "Error budget fast burn detected"
          description: "Consuming > 10% error budget in 1 hour. Current availability: {{ $value | humanizePercentage }}."
          runbook: "Immediate action required. Investigate and mitigate ongoing issues."

      # Error Budget: Medium burn (> 25% in 6 hours)
      - alert: ErrorBudgetMediumBurn
        expr: |
          (
            1 - (sum(rate(http_requests_total{status=~"2.."}[6h])) / sum(rate(http_requests_total[6h])))
          ) > 0.25
        for: 15m
        labels:
          severity: warning
          error_budget: medium_burn
        annotations:
          summary: "Error budget medium burn detected"
          description: "Consuming > 25% error budget in 6 hours. Current availability: {{ $value | humanizePercentage }}."
          runbook: "Review recent changes, implement stability improvements."

  - name: clipper_service_alerts
    interval: 30s
    rules:
      # Service Down
      - alert: ServiceDown
        expr: up == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Service {{ $labels.job }} is down"
          description: "{{ $labels.job }} has been down for more than 1 minute."

      # High Error Rate
      - alert: HighErrorRate
        expr: |
          (
            sum(rate(http_requests_total{status=~"5.."}[5m])) by (job)
            /
            sum(rate(http_requests_total[5m])) by (job)
          ) > 0.05
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High error rate on {{ $labels.job }}"
          description: "{{ $labels.job }} has an error rate of {{ $value | humanizePercentage }} over the last 5 minutes."

      # Critical Error Rate (P1)
      - alert: CriticalErrorRate
        expr: |
          (
            sum(rate(http_requests_total{status=~"5.."}[5m])) by (job)
            /
            sum(rate(http_requests_total[5m])) by (job)
          ) > 0.01
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Critical error rate on {{ $labels.job }}"
          description: "{{ $labels.job }} has a critical error rate of {{ $value | humanizePercentage }} (threshold: > 1%)."

      # High Response Time
      - alert: HighResponseTime
        expr: |
          histogram_quantile(0.95,
            sum(rate(http_request_duration_seconds_bucket[5m])) by (job, le)
          ) > 0.5
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High response time on {{ $labels.job }}"
          description: "{{ $labels.job }} 95th percentile response time is {{ $value }}s."

  - name: clipper_resource_alerts
    interval: 30s
    rules:
      # High CPU Usage
      - alert: HighCPUUsage
        expr: |
          100 - (avg by (instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High CPU usage on {{ $labels.instance }}"
          description: "CPU usage is {{ $value | humanize }}% on {{ $labels.instance }}."

      # High Memory Usage
      - alert: HighMemoryUsage
        expr: |
          (
            node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes
          ) / node_memory_MemTotal_bytes * 100 > 80
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High memory usage on {{ $labels.instance }}"
          description: "Memory usage is {{ $value | humanize }}% on {{ $labels.instance }}."

      # Low Disk Space
      - alert: LowDiskSpace
        expr: |
          (
            node_filesystem_avail_bytes{mountpoint="/"}
            / node_filesystem_size_bytes{mountpoint="/"}
          ) * 100 < 20
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Low disk space on {{ $labels.instance }}"
          description: "Disk space is {{ $value | humanize }}% free on {{ $labels.instance }}."

      # Critical Disk Space
      - alert: CriticalDiskSpace
        expr: |
          (
            node_filesystem_avail_bytes{mountpoint="/"}
            / node_filesystem_size_bytes{mountpoint="/"}
          ) * 100 < 10
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Critical disk space on {{ $labels.instance }}"
          description: "Disk space is {{ $value | humanize }}% free on {{ $labels.instance }}."

  - name: clipper_database_alerts
    interval: 30s
    rules:
      # Database Connection Issues
      - alert: DatabaseConnectionIssues
        expr: |
          sum(pg_stat_database_numbackends) by (datname) > 80
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High number of database connections"
          description: "Database {{ $labels.datname }} has {{ $value }} active connections."

      # Database Down
      - alert: DatabaseDown
        expr: pg_up == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "PostgreSQL is down"
          description: "PostgreSQL database is not responding."

      # Slow Queries
      - alert: SlowQueries
        expr: |
          rate(pg_stat_activity_max_tx_duration[5m]) > 30
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Slow database queries detected"
          description: "Database has queries taking more than 30 seconds."

  - name: clipper_redis_alerts
    interval: 30s
    rules:
      # Redis Down
      - alert: RedisDown
        expr: redis_up == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Redis is down"
          description: "Redis cache is not responding."

      # High Redis Memory Usage
      - alert: HighRedisMemoryUsage
        expr: |
          (redis_memory_used_bytes / redis_memory_max_bytes) * 100 > 80
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High Redis memory usage"
          description: "Redis is using {{ $value | humanize }}% of allocated memory."

      # Low Cache Hit Rate
      - alert: LowCacheHitRate
        expr: |
          (
            rate(redis_keyspace_hits_total[5m]) + rate(redis_keyspace_misses_total[5m])
          ) > 0.5
          and (
            rate(redis_keyspace_hits_total[5m])
            / (rate(redis_keyspace_hits_total[5m]) + rate(redis_keyspace_misses_total[5m]))
          ) < 0.8
        for: 30m
        labels:
          severity: info
        annotations:
          summary: "Low Redis cache hit rate"
          description: "Redis cache hit rate is {{ $value | humanizePercentage }} (only alerts when >0.5 req/s traffic)."

  - name: clipper_ssl_alerts
    interval: 1d
    rules:
      # SSL Certificate Expiring Soon
      - alert: SSLCertificateExpiringSoon
        expr: |
          (ssl_certificate_expiry_seconds - time()) / 86400 < 30
        for: 1h
        labels:
          severity: warning
        annotations:
          summary: "SSL certificate expiring soon"
          description: "SSL certificate for {{ $labels.domain }} expires in {{ $value }} days."

      # SSL Certificate Expired
      - alert: SSLCertificateExpired
        expr: |
          (ssl_certificate_expiry_seconds - time()) < 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "SSL certificate expired"
          description: "SSL certificate for {{ $labels.domain }} has expired."

  - name: clipper_search_alerts
    interval: 30s
    rules:
      # Semantic Search High Latency
      - alert: SemanticSearchHighLatency
        expr: |
          histogram_quantile(0.95, sum(rate(search_query_duration_ms_bucket{search_type="hybrid"}[5m])) by (le)) > 200
        for: 5m
        labels:
          severity: warning
          slo: search_latency
        annotations:
          summary: "Semantic search P95 latency exceeds 200ms"
          description: "Hybrid search P95 latency is {{ $value }}ms, target is < 200ms."
          runbook: "docs/operations/playbooks/search-incidents.md#high-latency"

      # Search Critical Latency
      - alert: SemanticSearchCriticalLatency
        expr: |
          histogram_quantile(0.95, sum(rate(search_query_duration_ms_bucket{search_type="hybrid"}[5m])) by (le)) > 500
        for: 3m
        labels:
          severity: critical
          slo: search_latency
        annotations:
          summary: "Semantic search P95 latency critical (>500ms)"
          description: "Hybrid search P95 latency is {{ $value }}ms. Immediate action required."
          runbook: "docs/operations/playbooks/search-incidents.md#critical-latency"

      # Embedding Generation Failures
      - alert: EmbeddingGenerationFailing
        expr: |
          rate(embedding_generation_errors_total[5m]) > 0.1
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Embedding generation failing"
          description: "Embedding generation error rate is {{ $value }}/sec. This affects semantic search quality."
          runbook: "docs/operations/playbooks/search-incidents.md#embedding-failures"

      # Low Embedding Coverage
      - alert: LowEmbeddingCoverage
        expr: |
          (clips_with_embeddings + clips_without_embeddings) > 0
          and clips_without_embeddings / (clips_with_embeddings + clips_without_embeddings) > 0.1
        for: 30m
        labels:
          severity: warning
        annotations:
          summary: "Low embedding coverage"
          description: "{{ $value | humanizePercentage }} of clips are missing embeddings (threshold: 10%)."
          runbook: "docs/operations/playbooks/search-incidents.md#low-coverage"

      # Embedding Cache Low Hit Rate
      - alert: EmbeddingCacheLowHitRate
        expr: |
          (rate(embedding_cache_hits_total[5m]) + rate(embedding_cache_misses_total[5m])) > 0
          and (rate(embedding_cache_hits_total[5m]) / (rate(embedding_cache_hits_total[5m]) + rate(embedding_cache_misses_total[5m]))) < 0.5
        for: 15m
        labels:
          severity: warning
        annotations:
          summary: "Embedding cache hit rate below 50%"
          description: "Embedding cache hit rate is {{ $value | humanizePercentage }}. This increases API costs and latency."
          runbook: "docs/operations/playbooks/search-incidents.md#cache-issues"

      # High Zero Result Rate
      - alert: HighZeroResultRate
        expr: |
          rate(search_queries_total[5m]) > 0
          and rate(search_zero_results_total[5m]) / rate(search_queries_total[5m]) > 0.1
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "High zero result rate in search"
          description: "{{ $value | humanizePercentage }} of searches return no results."
          runbook: "docs/operations/playbooks/search-incidents.md#zero-results"

      # Search Fallback Activated
      - alert: SearchFallbackActivated
        expr: |
          rate(search_fallback_total[5m]) > 0.5
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Search falling back to BM25 frequently"
          description: "Search is falling back to BM25-only at {{ $value }}/sec due to {{ $labels.reason }}."
          runbook: "docs/operations/playbooks/search-incidents.md#fallback-issues"

      # Indexing Job Failures
      - alert: IndexingJobFailing
        expr: |
          increase(indexing_jobs_total{status="failed"}[1h]) > 3
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Indexing jobs failing repeatedly"
          description: "{{ $value }} indexing jobs failed in the last hour."
          runbook: "docs/operations/playbooks/search-incidents.md#indexing-failures"

      # Vector Search Slow
      - alert: VectorSearchSlow
        expr: |
          histogram_quantile(0.95, sum(rate(vector_search_duration_ms_bucket[5m])) by (le)) > 100
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Vector re-ranking is slow"
          description: "Vector search P95 latency is {{ $value }}ms, target is < 100ms."
          runbook: "docs/operations/playbooks/search-incidents.md#vector-search-slow"

      # BM25 Search Slow
      - alert: BM25SearchSlow
        expr: |
          histogram_quantile(0.95, sum(rate(bm25_search_duration_ms_bucket[5m])) by (le)) > 50
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "BM25 candidate search is slow"
          description: "BM25 search P95 latency is {{ $value }}ms, target is < 50ms."
          runbook: "docs/operations/playbooks/search-incidents.md#bm25-search-slow"

      # Search Failover Rate High
      - alert: SearchFailoverRateHigh
        expr: |
          rate(search_fallback_total[5m]) > 5
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High search failover rate"
          description: "Search is failing over at {{ $value }}/sec due to {{ $labels.reason }}. OpenSearch may be degraded."
          runbook: "docs/operations/playbooks/search-incidents.md#search-failover"

      # Search Failover Rate Critical
      - alert: SearchFailoverRateCritical
        expr: |
          rate(search_fallback_total[5m]) > 20
        for: 3m
        labels:
          severity: critical
        annotations:
          summary: "Critical search failover rate"
          description: "Search is failing over at {{ $value }}/sec due to {{ $labels.reason }}. Immediate action required."
          runbook: "docs/operations/playbooks/search-incidents.md#search-failover"

      # Search Failover Latency High
      - alert: SearchFailoverLatencyHigh
        expr: |
          histogram_quantile(0.95, sum(rate(search_fallback_duration_ms_bucket[5m])) by (le)) > 500
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High search fallback latency"
          description: "P95 fallback path latency is {{ $value }}ms, may indicate PostgreSQL performance issues."
          runbook: "docs/operations/playbooks/search-incidents.md#fallback-latency"

  - name: clipper_webhook_alerts
    interval: 30s
    rules:
      # High Webhook Failure Rate
      - alert: HighWebhookFailureRate
        expr: |
          (
            sum(rate(webhook_delivery_total{status="failed"}[5m]))
            /
            sum(rate(webhook_delivery_total[5m]))
          ) > 0.1
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "High webhook delivery failure rate"
          description: "Webhook delivery failure rate is {{ $value | humanizePercentage }}, target is < 10%."
          runbook: "docs/backend/webhooks.md#troubleshooting"

      # Critical Webhook Failure Rate
      - alert: CriticalWebhookFailureRate
        expr: |
          (
            sum(rate(webhook_delivery_total{status="failed"}[5m]))
            /
            sum(rate(webhook_delivery_total[5m]))
          ) > 0.5
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Critical webhook delivery failure rate"
          description: "Webhook delivery failure rate is {{ $value | humanizePercentage }} (threshold: 50%)."
          runbook: "docs/backend/webhooks.md#troubleshooting"

      # Large Webhook Retry Queue
      - alert: LargeWebhookRetryQueue
        expr: webhook_retry_queue_size > 100
        for: 15m
        labels:
          severity: warning
        annotations:
          summary: "Large webhook retry queue"
          description: "Webhook retry queue has {{ $value }} items pending."
          runbook: "docs/backend/webhook-retry.md#high-retry-queue-size"

      # Critical Webhook Retry Queue
      - alert: CriticalWebhookRetryQueue
        expr: webhook_retry_queue_size > 500
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Critical webhook retry queue size"
          description: "Webhook retry queue has {{ $value }} items pending. Immediate action required."
          runbook: "docs/backend/webhook-retry.md#high-retry-queue-size"

      # Webhook Dead-Letter Queue Items
      - alert: WebhookDeadLetterQueueItems
        expr: webhook_dead_letter_queue_size > 10
        for: 30m
        labels:
          severity: warning
        annotations:
          summary: "Webhooks in dead-letter queue"
          description: "{{ $value }} webhook deliveries have failed permanently and are in the dead-letter queue."
          runbook: "docs/backend/webhook-retry.md#events-in-dlq"

      # Critical Webhook Dead-Letter Queue
      - alert: CriticalWebhookDeadLetterQueue
        expr: webhook_dead_letter_queue_size > 50
        for: 15m
        labels:
          severity: critical
        annotations:
          summary: "Critical number of webhooks in dead-letter queue"
          description: "{{ $value }} webhook deliveries have failed permanently. Investigation required."
          runbook: "docs/backend/webhook-retry.md#events-in-dlq"

      # High Webhook Delivery Latency
      - alert: HighWebhookDeliveryLatency
        expr: |
          histogram_quantile(0.95,
            sum(rate(webhook_delivery_duration_seconds_bucket{status="success"}[5m])) by (le)
          ) > 5
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "High webhook delivery latency"
          description: "P95 webhook delivery latency is {{ $value }}s, target is < 5s."
          runbook: "docs/backend/webhooks.md#troubleshooting"

      # Webhook Delivery Latency Spike
      - alert: WebhookDeliveryLatencySpike
        expr: |
          (
            histogram_quantile(0.95,
              sum(rate(webhook_delivery_duration_seconds_bucket{status="success"}[5m])) by (le)
            )
            /
            histogram_quantile(0.95,
              sum(rate(webhook_delivery_duration_seconds_bucket{status="success"}[1h])) by (le)
            )
          ) > 2
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Webhook delivery latency spike detected"
          description: "P95 latency has increased by {{ humanize $value }}x compared to 1-hour baseline."
          runbook: "docs/backend/webhooks.md#troubleshooting"

      # High DLQ Movement Rate
      - alert: HighDLQMovementRate
        expr: |
          sum(rate(webhook_dlq_movements_total[5m])) > 1
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "High rate of webhooks moving to DLQ"
          description: "{{ $value }} webhooks/sec are being moved to the dead-letter queue."
          runbook: "docs/backend/webhook-retry.md#high-dlq-movement-rate"

      # Critical DLQ Movement Rate
      - alert: CriticalDLQMovementRate
        expr: |
          sum(rate(webhook_dlq_movements_total[5m])) > 5
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Critical rate of webhooks moving to DLQ"
          description: "{{ $value }} webhooks/sec are being moved to the dead-letter queue. Immediate investigation required."
          runbook: "docs/backend/webhook-retry.md#high-dlq-movement-rate"

      # Subscription Consecutive Failures
      - alert: WebhookSubscriptionConsecutiveFailures
        expr: webhook_consecutive_failures_count > 5
        for: 15m
        labels:
          severity: warning
        annotations:
          summary: "Webhook subscription experiencing consecutive failures"
          description: "Subscription {{ $labels.subscription_id }} for event {{ $labels.event_type }} has {{ $value }} consecutive failures."
          runbook: "docs/backend/webhooks.md#subscription-health"

      # Subscription Critical Consecutive Failures
      - alert: WebhookSubscriptionCriticalFailures
        expr: webhook_consecutive_failures_count > 20
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Webhook subscription critically failing"
          description: "Subscription {{ $labels.subscription_id }} for event {{ $labels.event_type }} has {{ $value }} consecutive failures. Subscription may need attention."
          runbook: "docs/backend/webhooks.md#subscription-health"

      # Subscription Health Degradation
      - alert: WebhookSubscriptionHealthDegradation
        expr: |
          (
            sum(rate(webhook_subscription_delivery_total{status="failed"}[5m])) by (subscription_id)
            /
            sum(rate(webhook_subscription_delivery_total[5m])) by (subscription_id)
          ) > 0.5
        for: 15m
        labels:
          severity: warning
        annotations:
          summary: "Webhook subscription health degraded"
          description: "Subscription {{ $labels.subscription_id }} has {{ $value | humanizePercentage }} failure rate."
          runbook: "docs/backend/webhooks.md#subscription-health"

      # High Retry Exhaustion Rate
      - alert: HighRetryExhaustionRate
        expr: |
          (
            sum(rate(webhook_retry_attempts_count{final_status="failed"}[5m]))
            /
            sum(rate(webhook_retry_attempts_count[5m]))
          ) * 100 > 30
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "High webhook retry exhaustion rate"
          description: "{{ $value | humanizePercentage }} of webhook retries are exhausting all attempts."
          runbook: "docs/backend/webhook-retry.md#high-exhaustion-rate"

      # Webhook Delivery Stalled
      - alert: WebhookDeliveryStalled
        expr: |
          rate(webhook_delivery_total[5m]) == 0
          and webhook_retry_queue_size > 0
        for: 10m
        labels:
          severity: critical
        annotations:
          summary: "Webhook delivery processing appears stalled"
          description: "No webhook deliveries processed in 10 minutes, but retry queue has items."
          runbook: "docs/backend/webhooks.md#delivery-stalled"

      # No Active Webhook Subscriptions (informational)
      - alert: NoActiveWebhookSubscriptions
        expr: webhook_subscriptions_active == 0
        for: 1h
        labels:
          severity: info
        annotations:
          summary: "No active webhook subscriptions"
          description: "There are currently no active webhook subscriptions configured."
          runbook: "docs/backend/webhooks.md"

  - name: clipper_logging_alerts
    interval: 30s
    rules:
      # High Error Log Rate
      - alert: HighErrorLogRate
        expr: |
          sum(rate({level="error"}[5m])) by (service) > 10
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High error log rate in {{ $labels.service }}"
          description: "{{ $labels.service }} is generating {{ $value }} error logs per second."
          runbook: "Check logs for error patterns and investigate root cause."

      # Critical Error Spike
      - alert: CriticalErrorSpike
        expr: |
          sum(rate({level="error"}[5m])) by (service) > 50
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Critical error spike in {{ $labels.service }}"
          description: "{{ $labels.service }} is experiencing {{ $value }} errors/sec. Immediate action required."
          runbook: "Investigate error patterns, consider rollback if recent deployment."

      # Security Event: Failed Authentication
      - alert: FailedAuthenticationSpike
        expr: |
          sum(rate({message=~".*authentication failed.*|.*login failed.*|.*unauthorized.*"}[5m])) by (service) > 5
        for: 3m
        labels:
          severity: warning
          security: true
        annotations:
          summary: "Failed authentication spike in {{ $labels.service }}"
          description: "{{ $labels.service }} has {{ $value }} failed auth attempts per second."
          runbook: "Check for brute force attacks, review IP addresses, consider rate limiting."

      # Security Event: SQL Injection Attempt
      - alert: SQLInjectionAttempt
        expr: |
          count_over_time({level="error",message=~".*(?i)(union.*select|insert.*into|drop.*table|delete.*from).*"}[5m]) > 0
        for: 1m
        labels:
          severity: critical
          security: true
        annotations:
          summary: "Potential SQL injection attempt detected"
          description: "{{ $value }} potential SQL injection patterns detected in logs."
          runbook: "Review logs immediately, block suspicious IPs, verify input validation."

      # Security Event: Suspicious Activity
      - alert: SuspiciousSecurityEvent
        expr: |
          sum(rate({message=~".*(?i)(security|breach|intrusion|malicious|exploit).*",level="warn"}[5m])) by (service) > 1
        for: 2m
        labels:
          severity: critical
          security: true
        annotations:
          summary: "Suspicious security events in {{ $labels.service }}"
          description: "{{ $labels.service }} has {{ $value }} security-related warnings per second."
          runbook: "Investigate security logs, check for breach indicators, notify security team."

      # Application Panic/Crash
      - alert: ApplicationPanic
        expr: |
          count_over_time({message=~".*(?i)(panic|crash|fatal).*"}[5m]) > 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Application panic detected in {{ $labels.service }}"
          description: "{{ $value }} panic/crash events detected. Application may be unstable."
          runbook: "Check stack traces, review recent deployments, consider rollback."

      # No Logs Received
      - alert: NoLogsReceived
        expr: |
          absent_over_time(({job="backend"} or {job="frontend"})[10m])
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "No logs received from {{ $labels.job }}"
          description: "No logs have been received from {{ $labels.job }} for 10 minutes."
          runbook: "Check if application is running, verify log shipping configuration."

      # High Log Volume
      - alert: HighLogVolume
        expr: |
          sum(rate({job=~"backend|frontend"}[5m])) by (service) > 1000
        for: 10m
        labels:
          severity: info
        annotations:
          summary: "High log volume from {{ $labels.service }}"
          description: "{{ $labels.service }} is generating {{ $value }} logs per second."
          runbook: "Review if this is expected behavior or indicates an issue."

      # Repeated Error Pattern (optimized query)
      - alert: RepeatedErrorPattern
        expr: |
          count(count_over_time({level="error",service=~".+"} [1h])) by (service) > 100
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Repeated error pattern detected in {{ $labels.service }}"
          description: "{{ $labels.service }} has {{ $value }} error occurrences in the last hour."
          runbook: "Investigate root cause of repeated errors in the service."

      # Database Connection Errors in Logs
      - alert: DatabaseConnectionErrors
        expr: |
          sum(rate({message=~".*(?i)(database.*connection.*failed|postgres.*error|connection.*refused).*",level="error"}[5m])) by (service) > 1
        for: 3m
        labels:
          severity: critical
        annotations:
          summary: "Database connection errors in {{ $labels.service }}"
          description: "{{ $labels.service }} has {{ $value }} database connection errors per second."
          runbook: "Check database health, verify connection pool configuration."

      # Redis Connection Errors in Logs
      - alert: RedisConnectionErrors
        expr: |
          sum(rate({message=~".*(?i)(redis.*connection.*failed|redis.*error|redis.*timeout).*",level="error"}[5m])) by (service) > 1
        for: 3m
        labels:
          severity: warning
        annotations:
          summary: "Redis connection errors in {{ $labels.service }}"
          description: "{{ $labels.service }} has {{ $value }} Redis connection errors per second."
          runbook: "Check Redis health, verify connection configuration."

  - name: clipper_background_jobs_alerts
    interval: 30s
    rules:
      # Job Execution Failures
      - alert: BackgroundJobFailing
        expr: |
          (
            rate(job_execution_total{status="failed"}[10m])
            / (rate(job_execution_total{status="success"}[10m]) + rate(job_execution_total{status="failed"}[10m]))
          ) > 0.05
          and
          (
            (rate(job_execution_total{status="success"}[10m]) + rate(job_execution_total{status="failed"}[10m])) > 0
          )
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Background job {{ $labels.job_name }} is failing"
          description: "Job {{ $labels.job_name }} has a failure rate of {{ $value | humanizePercentage }} (threshold: 5% over the last 10 minutes)."
          runbook: "docs/operations/runbooks/background-jobs.md#job-failures"

      # Critical Job Failure Rate
      - alert: BackgroundJobCriticalFailureRate
        expr: |
          (
            rate(job_execution_total{status="failed"}[5m])
            / (rate(job_execution_total{status="success"}[5m]) + rate(job_execution_total{status="failed"}[5m]))
          ) > 0.5
          and
          (
            (rate(job_execution_total{status="success"}[5m]) + rate(job_execution_total{status="failed"}[5m])) > 0
          )
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Critical failure rate for job {{ $labels.job_name }}"
          description: "Job {{ $labels.job_name }} has a failure rate of {{ $value | humanizePercentage }} (threshold: 50%)."
          runbook: "docs/operations/runbooks/background-jobs.md#critical-failure-rate"

      # Job Not Running (Stale)
      # Note: 2-hour threshold works for most jobs (hot_score: 5m, trending_score: 60m, clip_sync: 15m)
      # but may not detect failures in infrequent jobs (reputation_tasks: 6h). Adjust threshold or
      # create job-specific alerts for jobs with intervals > 2 hours.
      - alert: BackgroundJobNotRunning
        expr: |
          (time() - job_last_success_timestamp_seconds) > 7200
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "Background job {{ $labels.job_name }} has not run successfully"
          description: "Job {{ $labels.job_name }} has not completed successfully for {{ $value | humanizeDuration }}."
          runbook: "docs/operations/runbooks/background-jobs.md#job-not-running"

      # Critical Job Stale (24 hours)
      - alert: BackgroundJobCriticallyStale
        expr: |
          (time() - job_last_success_timestamp_seconds) > 86400
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Background job {{ $labels.job_name }} critically stale"
          description: "Job {{ $labels.job_name }} has not completed successfully for {{ $value | humanizeDuration }} (>24 hours)."
          runbook: "docs/operations/runbooks/background-jobs.md#job-critically-stale"

      # High Job Duration
      - alert: BackgroundJobHighDuration
        expr: |
          histogram_quantile(0.95,
            sum(rate(job_execution_duration_seconds_bucket[10m])) by (job_name, le)
          ) > 300
        for: 15m
        labels:
          severity: warning
        annotations:
          summary: "Background job {{ $labels.job_name }} running slowly"
          description: "P95 duration for job {{ $labels.job_name }} is {{ $value }}s (threshold: 300s)."
          runbook: "docs/operations/runbooks/background-jobs.md#high-duration"

      # Critical Job Duration
      - alert: BackgroundJobCriticalDuration
        expr: |
          histogram_quantile(0.95,
            sum(rate(job_execution_duration_seconds_bucket[10m])) by (job_name, le)
          ) > 600
        for: 10m
        labels:
          severity: critical
        annotations:
          summary: "Background job {{ $labels.job_name }} critically slow"
          description: "P95 duration for job {{ $labels.job_name }} is {{ $value }}s (threshold: 600s / 10 minutes)."
          runbook: "docs/operations/runbooks/background-jobs.md#critical-duration"

      # Job Queue Growing
      - alert: BackgroundJobQueueGrowing
        expr: |
          job_queue_size > 100
          and (job_queue_size / ((job_queue_size offset 10m) or vector(0))) > 1.5
        for: 15m
        labels:
          severity: warning
        annotations:
          summary: "Job queue for {{ $labels.job_name }} is growing"
          description: "Queue size for job {{ $labels.job_name }} is {{ $value }} and growing by >50% over 10m."
          runbook: "docs/operations/runbooks/background-jobs.md#queue-growing"

      # Critical Job Queue Size
      - alert: BackgroundJobCriticalQueueSize
        expr: |
          job_queue_size > 1000
        for: 10m
        labels:
          severity: critical
        annotations:
          summary: "Critical queue size for {{ $labels.job_name }}"
          description: "Queue size for job {{ $labels.job_name }} is {{ $value }} (threshold: 1000)."
          runbook: "docs/operations/runbooks/background-jobs.md#critical-queue-size"

      # High Item Processing Failure Rate
      - alert: BackgroundJobHighItemFailureRate
        expr: |
          (
            rate(job_items_processed_total{status="failed"}[10m])
            / clamp_min(
              rate(job_items_processed_total{status="success"}[10m]) + rate(job_items_processed_total{status="failed"}[10m]),
              0.01
            )
          ) > 0.2
          and (
            rate(job_items_processed_total{status="success"}[10m]) + rate(job_items_processed_total{status="failed"}[10m])
          ) > 0
        for: 15m
        labels:
          severity: warning
        annotations:
          summary: "High item failure rate for job {{ $labels.job_name }}"
          description: "Job {{ $labels.job_name }} has {{ $value | humanizePercentage }} item failure rate (threshold: 20%)."
          runbook: "docs/operations/runbooks/background-jobs.md#high-item-failure-rate"

  # CDN Failover Alerts
  # Monitors CDN health and failover to origin
  - name: cdn_failover_alerts
    interval: 30s
    rules:
      # CDN Failover Rate High
      - alert: CDNFailoverRateHigh
        expr: rate(cdn_failover_total[5m]) > 5
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "CDN failover rate is high"
          description: "CDN is experiencing {{ $value | humanize }} failovers/sec for 5 minutes. Assets are being served from origin."
          runbook: "docs/operations/CDN_FAILOVER_RUNBOOK.md#cdn-outage-detected"

      # CDN Failover Rate Critical
      - alert: CDNFailoverRateCritical
        expr: rate(cdn_failover_total[5m]) > 20
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "CDN failover rate is critical"
          description: "CDN is experiencing {{ $value | humanize }} failovers/sec - immediate action required. Origin server may be overloaded."
          runbook: "docs/operations/CDN_FAILOVER_RUNBOOK.md#cdn-outage-detected"

      # CDN Failover Latency High
      - alert: CDNFailoverLatencyHigh
        expr: |
          histogram_quantile(0.95,
            sum by (le) (rate(cdn_failover_duration_ms_bucket[5m]))
          ) > 500
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "CDN failover latency is high"
          description: "P95 failover latency is {{ $value | humanize }}ms (threshold: 500ms). Users may experience slow asset loading."
          runbook: "docs/operations/CDN_FAILOVER_RUNBOOK.md#cdn-outage-detected"

      # CDN Timeout Errors
      - alert: CDNTimeoutErrors
        expr: rate(cdn_failover_total{reason="timeout"}[5m]) > 2
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High rate of CDN timeout errors"
          description: "CDN is timing out at {{ $value | humanize }} req/sec. May indicate CDN network issues."
          runbook: "docs/operations/CDN_FAILOVER_RUNBOOK.md#cdn-outage-detected"

      # Origin Server High Load (due to CDN failure)
      - alert: OriginServerHighLoad
        expr: |
          (
            rate(http_requests_total{source="origin"}[5m])
            / (rate(http_requests_total{source="cdn"}[5m]) + rate(http_requests_total{source="origin"}[5m]))
          ) > 0.5
          and rate(cdn_failover_total[5m]) > 5
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "Origin server handling high traffic due to CDN failure"
          description: "Origin is serving {{ $value | humanizePercentage }} of traffic (threshold: 50%). Consider scaling if sustained."
          runbook: "docs/operations/CDN_FAILOVER_RUNBOOK.md#if-origin-server-overloaded"

      # HLS Segment Delivery Failing
      - alert: HLSSegmentDeliveryFailing
        expr: |
          (
            rate(http_requests_total{path=~".*\\.ts",status=~"5.."}[5m])
            / rate(http_requests_total{path=~".*\\.ts"}[5m])
          ) > 0.05
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "High error rate for HLS segment delivery"
          description: "HLS segments failing at {{ $value | humanizePercentage }} (threshold: 5%). Video playback is degraded."
          runbook: "docs/operations/CDN_FAILOVER_RUNBOOK.md#cdn-outage-detected"

      # HLS Playlist Delivery Failing
      - alert: HLSPlaylistDeliveryFailing
        expr: |
          (
            rate(http_requests_total{path=~".*\\.m3u8",status=~"5.."}[5m])
            / rate(http_requests_total{path=~".*\\.m3u8"}[5m])
          ) > 0.05
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "High error rate for HLS playlist delivery"
          description: "HLS playlists failing at {{ $value | humanizePercentage }} (threshold: 5%). Video playback initiation is failing."
          runbook: "docs/operations/CDN_FAILOVER_RUNBOOK.md#cdn-outage-detected"

  # HPA Scaling Alerts
  # Monitors Horizontal Pod Autoscaler behavior and scaling events
  - name: hpa_scaling_alerts
    interval: 30s
    rules:
      # HPA at Maximum Replicas
      - alert: HPAMaxedOut
        expr: |
          kube_horizontalpodautoscaler_status_current_replicas{horizontalpodautoscaler=~"clipper-backend|clipper-frontend"}
          >= kube_horizontalpodautoscaler_spec_max_replicas{horizontalpodautoscaler=~"clipper-backend|clipper-frontend"}
        for: 15m
        labels:
          severity: warning
        annotations:
          summary: "HPA {{ $labels.horizontalpodautoscaler }} at maximum replicas"
          description: "HPA {{ $labels.horizontalpodautoscaler }} has been at max replicas ({{ $value }}) for 15 minutes. Consider increasing max replicas or investigating load."
          runbook: "docs/operations/runbooks/hpa-scaling.md#hpa-maxed-out"

      # HPA Unable to Scale
      - alert: HPAUnableToScale
        expr: |
          kube_horizontalpodautoscaler_status_condition{condition="ScalingLimited",status="true",horizontalpodautoscaler=~"clipper-backend|clipper-frontend"} == 1
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "HPA {{ $labels.horizontalpodautoscaler }} unable to scale"
          description: "HPA {{ $labels.horizontalpodautoscaler }} is unable to scale due to limits. Check resource constraints and pod quotas."
          runbook: "docs/operations/runbooks/hpa-scaling.md#unable-to-scale"

      # HPA Metrics Unavailable
      - alert: HPAMetricsUnavailable
        expr: |
          kube_horizontalpodautoscaler_status_condition{condition="ScalingActive",status="false",horizontalpodautoscaler=~"clipper-backend|clipper-frontend"} == 1
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "HPA {{ $labels.horizontalpodautoscaler }} metrics unavailable"
          description: "HPA {{ $labels.horizontalpodautoscaler }} cannot obtain metrics. Check metrics-server and Prometheus adapter health."
          runbook: "docs/operations/runbooks/hpa-scaling.md#metrics-unavailable"

      # HPA Frequent Scaling
      - alert: HPAFrequentScaling
        expr: |
          abs(
            delta(kube_horizontalpodautoscaler_status_desired_replicas{horizontalpodautoscaler=~"clipper-backend|clipper-frontend"}[10m])
          ) > 4
        for: 30m
        labels:
          severity: info
        annotations:
          summary: "HPA {{ $labels.horizontalpodautoscaler }} scaling frequently"
          description: "HPA {{ $labels.horizontalpodautoscaler }} has changed desired replicas by more than 4 in the last 10 minutes. This may indicate oscillating load or improper thresholds."
          runbook: "docs/operations/runbooks/hpa-scaling.md#frequent-scaling"

      # HPA Desired Replicas Mismatch
      - alert: HPADesiredReplicasMismatch
        expr: |
          abs(
            kube_horizontalpodautoscaler_status_desired_replicas{horizontalpodautoscaler=~"clipper-backend|clipper-frontend"}
            - kube_horizontalpodautoscaler_status_current_replicas{horizontalpodautoscaler=~"clipper-backend|clipper-frontend"}
          ) > 2
        for: 15m
        labels:
          severity: warning
        annotations:
          summary: "HPA {{ $labels.horizontalpodautoscaler }} replica mismatch"
          description: "HPA {{ $labels.horizontalpodautoscaler }} has a mismatch between desired and current replicas for 15m. Difference: {{ $value }}. Check pod scheduling and resource availability."
          runbook: "docs/operations/runbooks/hpa-scaling.md#replica-mismatch"

      # Custom Metrics Not Available
      - alert: HPACustomMetricsNotAvailable
        expr: |
          kube_horizontalpodautoscaler_status_condition{condition="AbleToScale",status="false",horizontalpodautoscaler=~"clipper-backend|clipper-frontend"}
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Custom metrics unavailable for {{ $labels.horizontalpodautoscaler }}"
          description: "HPA {{ $labels.horizontalpodautoscaler }} is unable to scale, possibly due to missing custom metrics. Check prometheus-adapter status."
          runbook: "docs/operations/runbooks/hpa-scaling.md#custom-metrics-unavailable"

      # Metrics Server Down
      - alert: MetricsServerDown
        expr: |
          kube_horizontalpodautoscaler_status_condition{condition="ScalingActive",status="false",horizontalpodautoscaler=~"clipper-backend|clipper-frontend"}
          and
          kube_horizontalpodautoscaler_status_condition{condition="AbleToScale",status="false",horizontalpodautoscaler=~"clipper-backend|clipper-frontend"}
        for: 3m
        labels:
          severity: critical
        annotations:
          summary: "Metrics Server is down"
          description: "Metrics Server is unavailable. HPA cannot function without resource metrics."
          runbook: "docs/operations/runbooks/hpa-scaling.md#metrics-server-down"

      # High Scale-Up Events
      - alert: HPAHighScaleUpRate
        expr: |
          delta(kube_horizontalpodautoscaler_status_desired_replicas{horizontalpodautoscaler=~"clipper-backend|clipper-frontend"}[5m]) > 3
        for: 10m
        labels:
          severity: info
        annotations:
          summary: "High scale-up rate for {{ $labels.horizontalpodautoscaler }}"
          description: "HPA {{ $labels.horizontalpodautoscaler }} has scaled up by more than 3 replicas in 5m. This may indicate sustained high load."
          runbook: "docs/operations/runbooks/hpa-scaling.md#high-scale-up-rate"

      # Backend at High Utilization Before Scaling
      - alert: BackendHighUtilizationBeforeScaling
        expr: |
          (
            sum(rate(http_requests_total{job="clipper-backend"}[2m]))
            / count(kube_pod_info{pod=~"clipper-backend.*"})
          ) > 900
          and kube_horizontalpodautoscaler_status_current_replicas{horizontalpodautoscaler="clipper-backend"}
          < kube_horizontalpodautoscaler_spec_max_replicas{horizontalpodautoscaler="clipper-backend"}
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Backend pods at high RPS before scaling"
          description: "Backend pods are handling >900 RPS (target: 1000) but HPA hasn't scaled yet. Check HPA responsiveness."
          runbook: "docs/operations/runbooks/hpa-scaling.md#slow-scale-up"

      # Frontend at High Utilization Before Scaling
      - alert: FrontendHighUtilizationBeforeScaling
        expr: |
          (
            sum(rate(http_requests_total{job="clipper-frontend"}[2m]))
            / count(kube_pod_info{pod=~"clipper-frontend.*"})
          ) > 900
          and kube_horizontalpodautoscaler_status_current_replicas{horizontalpodautoscaler="clipper-frontend"}
          < kube_horizontalpodautoscaler_spec_max_replicas{horizontalpodautoscaler="clipper-frontend"}
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Frontend pods at high RPS before scaling"
          description: "Frontend pods are handling >900 RPS (target: 1000) but HPA hasn't scaled yet. Check HPA responsiveness."
          runbook: "docs/operations/runbooks/hpa-scaling.md#slow-scale-up"
